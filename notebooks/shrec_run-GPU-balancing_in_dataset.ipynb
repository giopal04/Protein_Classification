{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PointNet to Classify Proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8ugd_8:R:3U_model1.vtk', '8h0v_18:R:c_model1.vtk', '3j3q_1:DX:4F_model1.vtk', '4u4u_23:XC:d1_model1.vtk', '6rny_4:H:H_model1.vtk']\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9244 entries, 0 to 9243\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   protein_id        9244 non-null   object\n",
      " 1   class_id          9244 non-null   int64 \n",
      " 2   number_of_points  9244 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 288.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>number_of_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ugd_8:R:3U_model1</td>\n",
       "      <td>96</td>\n",
       "      <td>5916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8h0v_18:R:c_model1</td>\n",
       "      <td>86</td>\n",
       "      <td>10078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3j3q_1:DX:4F_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>18432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4u4u_23:XC:d1_model1</td>\n",
       "      <td>83</td>\n",
       "      <td>8242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6rny_4:H:H_model1</td>\n",
       "      <td>34</td>\n",
       "      <td>9204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9239</th>\n",
       "      <td>3j3y_1:HL:6R_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>18342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9240</th>\n",
       "      <td>4u4y_15:O:C3_model1</td>\n",
       "      <td>91</td>\n",
       "      <td>12976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9241</th>\n",
       "      <td>7w31_26:AA:c_model1</td>\n",
       "      <td>18</td>\n",
       "      <td>16454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>3j4k_1:E:E_model1</td>\n",
       "      <td>90</td>\n",
       "      <td>23188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9243</th>\n",
       "      <td>3j3y_1:UIA:fE_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>17888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9244 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                protein_id  class_id  number_of_points\n",
       "0       8ugd_8:R:3U_model1        96              5916\n",
       "1       8h0v_18:R:c_model1        86             10078\n",
       "2      3j3q_1:DX:4F_model1         8             18432\n",
       "3     4u4u_23:XC:d1_model1        83              8242\n",
       "4        6rny_4:H:H_model1        34              9204\n",
       "...                    ...       ...               ...\n",
       "9239   3j3y_1:HL:6R_model1         8             18342\n",
       "9240   4u4y_15:O:C3_model1        91             12976\n",
       "9241   7w31_26:AA:c_model1        18             16454\n",
       "9242     3j4k_1:E:E_model1        90             23188\n",
       "9243  3j3y_1:UIA:fE_model1         8             17888\n",
       "\n",
       "[9244 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data\n",
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "\n",
    "home_path = '/home/giorgio/venvs/SHREC'\n",
    "if home_path not in sys.path:\n",
    "    sys.path.append(home_path)\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datasetStudy import *\n",
    "\n",
    "debug_pointnet = False\n",
    "\n",
    "# Multiple function here depends on this variable\n",
    "#root = '/mnt/dataset/shrec-2025-protein-classification/v2-20250331' \n",
    "base_path = Path('/mnt/')\n",
    "#base_path = Path('/mnt/raid1')\n",
    "root = base_path / 'dataset/shrec-2025-protein-classification/v2-20250331'\n",
    "\n",
    "#train_data = os.listdir(os.path.join(root, 'train'))\n",
    "#train_data_cls = pd.read_csv('datasets/train_set-all.csv', sep=',', index_col=0)\n",
    "\n",
    "train_data = os.listdir(root / 'train')\n",
    "train_data_cls = pd.read_csv('../datasets/train_set-all.csv', sep=',', index_col=0)\n",
    "\n",
    "print(train_data[:5])\n",
    "print()\n",
    "print(train_data_cls.info())\n",
    "train_data_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = root / 'train-xz'\n",
    "mountpoint = '/tmp/ramdrive'\n",
    "size_in_gb = 3\n",
    "\n",
    "def create_ramdrive(src, mountpoint, size_in_gb):\n",
    "    if Path(mountpoint).exists() or Path(mountpoint).is_dir():\n",
    "        print(f'Ramdrive already created, exiting...')\n",
    "        return\n",
    "    \n",
    "    !sudo mkdir -p \"{mountpoint}\"\n",
    "    !sudo mount -o size=\"{size_in_gb}\"G -t tmpfs none \"{mountpoint}\"\n",
    "    !rsync -a --progress \"{src}\" \"{mountpoint}\"\n",
    "    \n",
    "def unmount_ramdrive(mountpoint):\n",
    "    !sudo umount \"{mountpoint}\"\n",
    "    !sudo rmdir \"{mountpoint}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramdrive already created, exiting...\n"
     ]
    }
   ],
   "source": [
    "create_ramdrive(src, mountpoint, size_in_gb)\n",
    "#unmount_ramdrive(mountpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>number_of_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [protein_id, class_id, number_of_points]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_disconnected_mesh(train_data_cls, 96, False)\n",
    "\n",
    "disconnected_dict = {}\n",
    "for idx in range(97):\n",
    "    disconnected_dict[idx] = possible_disconnected_mesh(train_data_cls, idx)\n",
    "\n",
    "#print(disconnected_dict)\n",
    "\n",
    "_, damaged = possible_disconnected_mesh(train_data_cls, 8, True); damaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes which have between 0 and 5 element: 16/97\n"
     ]
    }
   ],
   "source": [
    "dist = cls_distribution(train_data_cls)\n",
    "inspect_distribution(dist, l_lim=0, u_lim=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and save new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls 0: 101\t101\t101\t0\t0\t101\t101\t\n",
      "cls 1: 3\t3\t0\t0\t0\t0\t3\t\n",
      "cls 2: 17\t17\t17\t17\t17\t0\t17\t\n",
      "cls 3: 21\t21\t21\t21\t21\t0\t21\t\n",
      "cls 4: 15\t15\t15\t15\t15\t0\t15\t\n",
      "cls 5: 127\t127\t127\t0\t0\t127\t127\t\n",
      "cls 6: 25\t25\t25\t25\t25\t0\t25\t\n",
      "cls 7: 75\t75\t75\t75\t0\t0\t75\t\n",
      "cls 8: 2054\t2054\t2054\t0\t0\t2054\t0\t\n",
      "cls 9: 66\t66\t66\t66\t0\t0\t66\t\n",
      "cls 10: 14\t14\t14\t14\t14\t0\t14\t\n",
      "cls 11: 43\t43\t43\t43\t43\t0\t43\t\n",
      "cls 12: 10\t10\t10\t10\t10\t0\t10\t\n",
      "cls 13: 22\t22\t22\t22\t22\t0\t22\t\n",
      "cls 14: 489\t489\t489\t0\t0\t489\t0\t\n",
      "cls 15: 89\t89\t89\t89\t0\t0\t89\t\n",
      "cls 16: 154\t154\t154\t0\t0\t154\t154\t\n",
      "cls 17: 116\t116\t116\t0\t0\t116\t116\t\n",
      "cls 18: 76\t76\t76\t76\t0\t0\t76\t\n",
      "cls 19: 42\t42\t42\t42\t42\t0\t42\t\n",
      "cls 20: 7\t7\t0\t0\t0\t0\t7\t\n",
      "cls 21: 74\t74\t74\t74\t0\t0\t74\t\n",
      "cls 22: 58\t58\t58\t58\t0\t0\t58\t\n",
      "cls 23: 10\t10\t10\t10\t10\t0\t10\t\n",
      "cls 24: 10\t10\t10\t10\t10\t0\t10\t\n",
      "cls 25: 143\t143\t143\t0\t0\t143\t143\t\n",
      "cls 26: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 27: 3\t3\t0\t0\t0\t0\t3\t\n",
      "cls 28: 18\t18\t18\t18\t18\t0\t18\t\n",
      "cls 29: 14\t14\t14\t14\t14\t0\t14\t\n",
      "cls 30: 3\t3\t0\t0\t0\t0\t3\t\n",
      "cls 31: 33\t33\t33\t33\t33\t0\t33\t\n",
      "cls 32: 93\t93\t93\t93\t0\t0\t93\t\n",
      "cls 33: 126\t126\t126\t0\t0\t126\t126\t\n",
      "cls 34: 73\t73\t73\t73\t0\t0\t73\t\n",
      "cls 35: 23\t23\t23\t23\t23\t0\t23\t\n",
      "cls 36: 8\t8\t0\t0\t0\t0\t8\t\n",
      "cls 37: 119\t119\t119\t0\t0\t119\t119\t\n",
      "cls 38: 34\t34\t34\t34\t34\t0\t34\t\n",
      "cls 39: 19\t19\t19\t19\t19\t0\t19\t\n",
      "cls 40: 95\t95\t95\t95\t0\t0\t95\t\n",
      "cls 41: 95\t95\t95\t95\t0\t0\t95\t\n",
      "cls 42: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 43: 70\t70\t70\t70\t0\t0\t70\t\n",
      "cls 44: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 45: 109\t109\t109\t0\t0\t109\t109\t\n",
      "cls 46: 44\t44\t44\t44\t44\t0\t44\t\n",
      "cls 47: 37\t37\t37\t37\t37\t0\t37\t\n",
      "cls 48: 27\t27\t27\t27\t27\t0\t27\t\n",
      "cls 49: 49\t49\t49\t49\t49\t0\t49\t\n",
      "cls 50: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 51: 71\t71\t71\t71\t0\t0\t71\t\n",
      "cls 52: 76\t76\t76\t76\t0\t0\t76\t\n",
      "cls 53: 53\t53\t53\t53\t0\t0\t53\t\n",
      "cls 54: 128\t128\t128\t0\t0\t128\t128\t\n",
      "cls 55: 26\t26\t26\t26\t26\t0\t26\t\n",
      "cls 56: 654\t654\t654\t0\t0\t654\t0\t\n",
      "cls 57: 9\t9\t0\t0\t0\t0\t9\t\n",
      "cls 58: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 59: 54\t54\t54\t54\t0\t0\t54\t\n",
      "cls 60: 73\t73\t73\t73\t0\t0\t73\t\n",
      "cls 61: 240\t240\t240\t0\t0\t240\t240\t\n",
      "cls 62: 135\t135\t135\t0\t0\t135\t135\t\n",
      "cls 63: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 64: 57\t57\t57\t57\t0\t0\t57\t\n",
      "cls 65: 10\t10\t10\t10\t10\t0\t10\t\n",
      "cls 66: 54\t54\t54\t54\t0\t0\t54\t\n",
      "cls 67: 17\t17\t17\t17\t17\t0\t17\t\n",
      "cls 68: 5\t5\t0\t0\t0\t0\t5\t\n",
      "cls 69: 80\t80\t80\t80\t0\t0\t80\t\n",
      "cls 70: 223\t223\t223\t0\t0\t223\t223\t\n",
      "cls 71: 102\t102\t102\t0\t0\t102\t102\t\n",
      "cls 72: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 73: 6\t6\t0\t0\t0\t0\t6\t\n",
      "cls 74: 150\t150\t150\t0\t0\t150\t150\t\n",
      "cls 75: 69\t69\t69\t69\t0\t0\t69\t\n",
      "cls 76: 10\t10\t10\t10\t10\t0\t10\t\n",
      "cls 77: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 78: 5\t5\t0\t0\t0\t0\t5\t\n",
      "cls 79: 78\t78\t78\t78\t0\t0\t78\t\n",
      "cls 80: 57\t57\t57\t57\t0\t0\t57\t\n",
      "cls 81: 58\t58\t58\t58\t0\t0\t58\t\n",
      "cls 82: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 83: 115\t115\t115\t0\t0\t115\t115\t\n",
      "cls 84: 18\t18\t18\t18\t18\t0\t18\t\n",
      "cls 85: 49\t49\t49\t49\t49\t0\t49\t\n",
      "cls 86: 203\t203\t203\t0\t0\t203\t203\t\n",
      "cls 87: 103\t103\t103\t0\t0\t103\t103\t\n",
      "cls 88: 97\t97\t97\t97\t0\t0\t97\t\n",
      "cls 89: 3\t3\t0\t0\t0\t0\t3\t\n",
      "cls 90: 788\t788\t788\t0\t0\t788\t0\t\n",
      "cls 91: 118\t118\t118\t0\t0\t118\t118\t\n",
      "cls 92: 129\t129\t129\t0\t0\t129\t129\t\n",
      "cls 93: 61\t61\t61\t61\t0\t0\t61\t\n",
      "cls 94: 52\t52\t52\t52\t0\t0\t52\t\n",
      "cls 95: 2\t2\t0\t0\t0\t0\t2\t\n",
      "cls 96: 35\t35\t35\t35\t35\t0\t35\t\n",
      "len(train_data_cls) = 9244\n",
      "len(raw_train_dataframe_f1) = 9244\n",
      "len(raw_train_dataframe_f2) = 9172\n",
      "len(raw_train_dataframe_f3) = 2546\n",
      "len(raw_train_dataframe_f4) = 692\n",
      "len(raw_train_dataframe_f5) = 6626\n"
     ]
    }
   ],
   "source": [
    "dist_all = cls_distribution(train_data_cls)\n",
    "\n",
    "# Filter the number of points\n",
    "raw_train_dataframe_f1 = number_of_point_filter(train_data_cls, 1000)\n",
    "dist_f1 = cls_distribution(raw_train_dataframe_f1)\n",
    "\n",
    "\n",
    "# Filter according to the number of samples in classes\n",
    "raw_train_dataframe_f2 = number_of_class_filter(raw_train_dataframe_f1, l_cut_off=10)\n",
    "dist_f2 = cls_distribution(raw_train_dataframe_f2)\n",
    "\n",
    "raw_train_dataframe_f3 = number_of_class_filter(raw_train_dataframe_f1, l_cut_off=10, u_cut_off=100)\n",
    "dist_f3 = cls_distribution(raw_train_dataframe_f3)\n",
    "\n",
    "raw_train_dataframe_f4 = number_of_class_filter(raw_train_dataframe_f1, l_cut_off=10, u_cut_off=50)\n",
    "dist_f4 = cls_distribution(raw_train_dataframe_f4)\n",
    "\n",
    "raw_train_dataframe_f5 = number_of_class_filter(raw_train_dataframe_f1, l_cut_off=100)\n",
    "dist_f5 = cls_distribution(raw_train_dataframe_f5)\n",
    "\n",
    "raw_train_dataframe_f6 = number_of_class_filter(raw_train_dataframe_f1, l_cut_off=0, u_cut_off=450)\n",
    "dist_f6 = cls_distribution(raw_train_dataframe_f6)\n",
    "\n",
    "distributions = [dist_all, dist_f1, dist_f2, dist_f3, dist_f4, dist_f5, dist_f6]\n",
    "for idx in range(len(dist_all)):\n",
    "    output = f'cls {idx}: '\n",
    "    for dist in distributions:\n",
    "        output += f'{print_dist(dist, idx)}\\t'\n",
    "    print(output)\n",
    "\n",
    "print(f'{len(train_data_cls) = }')\n",
    "print(f'{len(raw_train_dataframe_f1) = }')\n",
    "print(f'{len(raw_train_dataframe_f2) = }')\n",
    "print(f'{len(raw_train_dataframe_f3) = }')\n",
    "print(f'{len(raw_train_dataframe_f4) = }')\n",
    "print(f'{len(raw_train_dataframe_f5) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Class 40 has only 95 proteins, but 10000 were requested. Using all available proteins.\n",
      "Warning: Class 88 has only 97 proteins, but 10000 were requested. Using all available proteins.\n",
      "Warning: Class 0 has only 101 proteins, but 1000 were requested. Using all available proteins.\n",
      "Warning: Class 1 has only 3 proteins, but 1000 were requested. Using all available proteins.\n"
     ]
    }
   ],
   "source": [
    "cls_selected = [8, 90, 56, 14, 61, 70, 86, 5, 16, 25, 54, 62, 74, 83, 91, 92, 17, 45, 87, 71]\n",
    "\n",
    "df_3 = create_dataframe(raw_train_dataframe_f1, class_ids=cls_selected[:2],  number_of_proteins=500)\n",
    "df_4 = create_dataframe(raw_train_dataframe_f1, class_ids=cls_selected[:10], number_of_proteins=100)\n",
    "df_5 = create_dataframe(raw_train_dataframe_f1, class_ids=cls_selected[:20], number_of_proteins=50)\n",
    "df_6 = create_dataframe(raw_train_dataframe_f1, class_ids=cls_selected[:20], number_of_proteins=100)\n",
    "#df_7 = create_dataframe(raw_train_dataframe_f6, class_ids=dist_f6_available_classes, number_of_proteins=100000)\n",
    "df_8 = create_dataframe(raw_train_dataframe_f1, class_ids=[40, 88], number_of_proteins=10000)\n",
    "df_9 = create_dataframe(raw_train_dataframe_f1, class_ids=[0, 1], number_of_proteins=1000)\n",
    "\n",
    "'''\n",
    "df_3.to_csv('datasets/train_set-2_cls-1000_images.csv')\n",
    "df_4.to_csv('datasets/train_set-10_cls-1000_images.csv')\n",
    "df_5.to_csv('datasets/train_set-20_cls-1000_images.csv')\n",
    "df_6.to_csv('datasets/train_set-20_cls-2000_images.csv')\n",
    "df_7.to_csv('datasets/train_set-all-cls_except-cls-larger-than-450.csv')\n",
    "symmetria_df.to_csv('datasets/train_set-symmetria-100k-easy-astroid-citrus-10k-samples.csv')\n",
    "df_8.to_csv('datasets/train_set-2_cls_40_88-192_proteins.csv')\n",
    "df_9.to_csv('../datasets/train_set_unbalanced-2_cls_0_1.csv')\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulize protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "PolyData (0x751cddf01720)\n",
      "  N Cells:    11576\n",
      "  N Points:   5788\n",
      "  N Strips:   0\n",
      "  X Bounds:   -1.384e+01, 1.552e+01\n",
      "  Y Bounds:   -1.553e+01, 1.623e+01\n",
      "  Z Bounds:   -1.842e+01, 2.068e+01\n",
      "  N Arrays:   3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ee45ddd61048d7a9d8e6e70b1c6781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value='<iframe src=\"http://localhost:36287/index.html?ui=P_0x751cde951b80_3&reconnect=auto\" class=\"pyviâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_id = 1\n",
    "df = raw_train_dataframe_f1[raw_train_dataframe_f1['class_id'] == cls_id]\n",
    "print(len(df))\n",
    "visualize_mesh(raw_train_dataframe_f1, cls=cls_id, idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format import Text\n",
    "import torch\n",
    "from symmetria.transformations import *\n",
    "from symmetria.shapes import BenchmarkShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rectangle = np.array([[0, 2, 0, 0, 2, 2, 0, 2],\n",
    "                             [0, 0, 1, 0, 1, 0, 1, 1],\n",
    "                             [0, 0, 0, 1, 0, 1, 1, 1]]).astype(np.float32)\n",
    "\n",
    "points_rectangle_t = torch.from_numpy(points_rectangle)\n",
    "\n",
    "rot = random_rotation_matrix(rotate_x=False, rotate_z=False)\n",
    "\n",
    "shapebench = BenchmarkShape(points_rectangle_t)\n",
    "#shapebench.apply_rotation(torch.from_numpy(rot))\n",
    "#shapebench.apply_traslation(1,1,0)\n",
    "#shapebench.apply_uniform_noise(100, 1)\n",
    "#shapebench.apply_gaussian_noise(100, 1)\n",
    "\n",
    "rect = pv.PolyData(shapebench.points.numpy().T)\n",
    "#rect.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class RotateAroundZero():\n",
    "    def __init__(self, p=0.5, rot=None):\n",
    "        self.p = p\n",
    "        self.rot = rot\n",
    "\n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            if self.rot == None:\n",
    "                rot = torch.from_numpy(random_rotation_matrix())\n",
    "            else:\n",
    "                rot = self.rot\n",
    "        \n",
    "            rot = rot.to(torch.device(points.device))\n",
    "\n",
    "            ones = torch.ones((1,points.shape[0]), device=points.device)\n",
    "            coords = torch.concatenate((torch.transpose(points, 0, 1), ones))\n",
    "            \n",
    "            coords = rot@coords\n",
    "            \n",
    "            points = torch.transpose(coords[:3,:], 0, 1)\n",
    "\n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'RotateAroundZero(p={repr(self.p)}, rot={repr(self.rot)})'\n",
    "    \n",
    "class Translate():\n",
    "    def __init__(self, p=0.5, shift=None, scale=1):\n",
    "        self.p = p\n",
    "        self.shift = shift\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, points):        \n",
    "        if random() < self.p:\n",
    "            if self.shift == None:\n",
    "                shift = (torch.rand(1, device=points.device) * self.scale,\n",
    "                         torch.rand(1, device=points.device) * self.scale,\n",
    "                         torch.rand(1, device=points.device) * self.scale)\n",
    "            else:\n",
    "                shift = self.shift\n",
    "            \n",
    "            for i in range(len(shift)):\n",
    "                points[:,i] += shift[i]\n",
    "                   \n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Translate(p={repr(self.p)}, shift={repr(self.shift)}, scale={repr(self.scale)})'\n",
    "    \n",
    "class UniformNoise():\n",
    "    def __init__(self, n, T, p=0.5):\n",
    "        self.p = p\n",
    "        self.n, self.T = n, T\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            num_points = points.shape[0]\n",
    "\n",
    "            indices = torch.randperm(num_points, device=points.device)[:self.T]\n",
    "\n",
    "            points[indices,:] = points[indices,:] + (2*torch.rand(self.T, 3, device=points.device)-1)/self.n\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'UniformNoise(n={repr(self.n)}, T={repr(self.T)}, p={repr(self.p)})'\n",
    "    \n",
    "class GaussianNoise():\n",
    "    def __init__(self, n, T, p=0.5):\n",
    "        self.p = p\n",
    "        self.n, self.T = n, T\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            num_points = points.shape[0]\n",
    "\n",
    "            indices = torch.randperm(num_points, device=points.device)[:self.T]\n",
    "    \n",
    "            points[indices,:] = points[indices,:] + torch.rand(self.T, 3, device=points.device)/self.n\n",
    "\n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GaussianNoise(n={repr(self.n)}, T={repr(self.T)}, p={repr(self.p)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate(p=1, shift=None, scale=1)\n",
      "Translate(p=1, shift=None, scale=1)\n"
     ]
    }
   ],
   "source": [
    "points_rectangle = np.array([[0, 2, 0, 0, 2, 2, 0, 2],\n",
    "                             [0, 0, 1, 0, 1, 0, 1, 1],\n",
    "                             [0, 0, 0, 1, 0, 1, 1, 1]]).astype(np.float32).T\n",
    "\n",
    "points_rectangle_t = torch.from_numpy(points_rectangle).to(torch.device('cuda:0'))\n",
    "\n",
    "translate = Translate(p=1)\n",
    "print(translate)\n",
    "points_rectangle_t = translate(points_rectangle_t)\n",
    "print(translate)\n",
    "rect = pv.PolyData(points_rectangle)\n",
    "#rect.plot()\n",
    "\n",
    "rect_tfm = pv.PolyData(points_rectangle_t.cpu().numpy())\n",
    "#rect_tfm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_train_valid_set_boring(df, val_pct, seed=42):\\n    train_size = int(len(df) * val_pct)    \\n    df_train = df.sample(train_size, random_state=seed)\\n    df_valid = df.drop(df_train.index)\\n\\n    return df_train, df_valid\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lzma\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from symmetria.transforms.RandomSampler import RandomSampler\n",
    "from symmetria.transforms.UnitSphereNormalization import UnitSphereNormalization\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_df, tfms, balancing=False, root=root, extention='xz', train=True):\n",
    "        super().__init__()\n",
    "        self.df = data_df\n",
    "        self.tfms = tfms\n",
    "        self.balancing = balancing\n",
    "        self.extention = extention\n",
    "        \n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            if self.extention == 'vtk':\n",
    "                self.root = os.path.join(root, 'train')\n",
    "\n",
    "            elif self.extention == 'xz':\n",
    "                #self.root = os.path.join(root, 'train-symmetria-xz') # TODO roll back\n",
    "                self.root = os.path.join(root, 'train-xz')\n",
    "            else:\n",
    "                raise UserWarning('Extention not supported')\n",
    "            \n",
    "        else:\n",
    "            raise UserWarning('Not yet implemented for the test dataset')\n",
    "        \n",
    "        self.encode_label()\n",
    "        self.loader()\n",
    "    \n",
    "    def loader(self):\n",
    "        self.data = []\n",
    "        \n",
    "        # Defining the balancing factor for each class\n",
    "        self.balancing_factors = {}\n",
    "        dist = cls_distribution(df)\n",
    "        for key, value in dist.items():\n",
    "            if self.balancing:\n",
    "                self.balancing_factors[key] = max_of_dist(dist)//value\n",
    "            \n",
    "            else:\n",
    "                self.balancing_factors[key] = int(1)\n",
    "        \n",
    "        # Loading data\n",
    "        for index in tqdm(self.df.index):\n",
    "            protein, cls, nop = self.df['protein_id'].loc[index], self.df['class_id'].loc[index], self.df['number_of_points'].loc[index]\n",
    "\n",
    "            cls_t = torch.tensor(self.encoded_cls[cls]).to(torch.device('cuda:0'))\n",
    "\n",
    "            if self.extention == 'vtk':\n",
    "                point_cloud = self.get_vtk_points(protein)\n",
    "\n",
    "            elif self.extention == 'xz':\n",
    "                point_cloud = self.get_xz_points(protein, cls, nop)\n",
    "\n",
    "            point_cloud_t = torch.from_numpy(point_cloud).to(torch.device('cuda:0'))     \n",
    "\n",
    "            point_cloud_t, cls_t = point_cloud_t.type(torch.float32), cls_t.type(torch.float32)\n",
    "            \n",
    "            for _ in range(self.balancing_factors[cls]):\n",
    "                self.data.append((point_cloud_t, cls_t))\n",
    "\n",
    "    def get_vtk_points(self, name):\n",
    "        prot_file = name + '.' + self.extention\n",
    "        prot_file = os.path.join(self.root, prot_file)\n",
    "        \n",
    "        prot_mesh = pv.read(prot_file)\n",
    "\n",
    "        return prot_mesh.points\n",
    "\n",
    "    def get_xz_points(self, name, cls, nop=None, debug=False):\n",
    "        cls = str(cls)\n",
    "        nop = str(nop) if nop and nop != 'nan' else None\n",
    "\n",
    "        while len(cls) < 2:\n",
    "            cls = '0' + cls\n",
    "        \n",
    "        if nop and nop != 'nan':\n",
    "            while len(nop) < 6:\n",
    "                nop = '0' + nop\n",
    "\n",
    "        if nop and nop != 'nan':\n",
    "            prot_file = cls + '-' + nop +  '-' + name.replace(':', '+') + '.' + self.extention\n",
    "        else:\n",
    "            prot_file = cls + '-' + name.replace(':', '+') + '.' + self.extention\n",
    "        if debug:\n",
    "            print(f'ProteinDataset is loading files from {self.root}')\n",
    "        prot_file = os.path.join(self.root, prot_file)        \n",
    "        \n",
    "        with lzma.open(prot_file, 'rt') as f:\n",
    "            point_cloud = np.loadtxt(f)\n",
    "\n",
    "        return point_cloud\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index, do_transform=True):\n",
    "        prot, cls = self.data[index]\n",
    "        \n",
    "        if do_transform:\n",
    "            for tfm in self.tfms:\n",
    "                prot = tfm(prot)\n",
    "\n",
    "        # NOTE: here we transpose the points (from Nx3 to 3xN) to then pass them to the PoinNet encoder\n",
    "        prot = torch.transpose(prot, 0, 1)\n",
    "        return (prot, cls)\n",
    "    \n",
    "    def encode_label(self):\n",
    "        self.encoded_cls = {}\n",
    "        \n",
    "        prot_clss = np.sort(self.df['class_id'].unique())\n",
    "        \n",
    "        #One hot encoding\n",
    "        '''\n",
    "        for idx, cls in enumerate(prot_clss):\n",
    "            self.encoded_cls[cls] = np.eye(len(prot_clss))[idx]\n",
    "        '''\n",
    "\n",
    "        for idx, cls in enumerate(prot_clss):\n",
    "            self.encoded_cls[cls] = int(idx)\n",
    "\n",
    "    def render_pointcloud(self, index):\n",
    "        prot, _ = self.data[index]\n",
    "        prot = torch.transpose(prot, 0, 1).numpy()\n",
    "        cloud = pv.PolyData(prot)\n",
    "        print(cloud)\n",
    "        cloud.plot()\n",
    "    \n",
    "\n",
    "transforms = [UnitSphereNormalization(),\n",
    "              Translate(p=0.8, scale=1),\n",
    "              RotateAroundZero(p=0.8),\n",
    "              GaussianNoise(n=100, T=2500, p=0.8),\n",
    "              RandomSampler(sample_size=5000)]\n",
    "\n",
    "\n",
    "def generate_train_valid_set(df, tfms, val_pct, root=root, seed=42, **kwargs):\n",
    "    '''\n",
    "    Can also take some other arguments to be passed to the dataset initializer\n",
    "\n",
    "            -> path (str): path to the parent directory containing the train files \n",
    "    '''\n",
    "\n",
    "    train_size = int(len(df) * (1 - val_pct))    \n",
    "    df_train = df.sample(train_size, random_state=seed)\n",
    "    df_valid = df.drop(df_train.index)\n",
    "\n",
    "    return ProteinDataset(df_train, tfms, root=root, **kwargs), ProteinDataset(df_valid, tfms, root=root, **kwargs)\n",
    "\n",
    "'''\n",
    "def generate_train_valid_set_boring(df, val_pct, seed=42):\n",
    "    train_size = int(len(df) * val_pct)    \n",
    "    df_train = df.sample(train_size, random_state=seed)\n",
    "    df_valid = df.drop(df_train.index)\n",
    "\n",
    "    return df_train, df_valid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Class 2 has only 17 proteins, but 100000 were requested. Using all available proteins.\n",
      "Warning: Class 30 has only 3 proteins, but 100000 were requested. Using all available proteins.\n",
      "Warning: Class 10 has only 14 proteins, but 100000 were requested. Using all available proteins.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "np.int64(10)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m example_dataframe = create_dataframe(raw_train_dataframe_f1, class_ids=[\u001b[32m2\u001b[39m, \u001b[32m30\u001b[39m, \u001b[32m10\u001b[39m], number_of_proteins=\u001b[32m100000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m example_set_train, example_set_valid = \u001b[43mgenerate_train_valid_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_dataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mgenerate_train_valid_set\u001b[39m\u001b[34m(df, tfms, val_pct, root, seed, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m df_train = df.sample(train_size, random_state=seed)\n\u001b[32m    149\u001b[39m df_valid = df.drop(df_train.index)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mProteinDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, ProteinDataset(df_valid, tfms, root=root, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mProteinDataset.__init__\u001b[39m\u001b[34m(self, data_df, tfms, balancing, root, extention, train)\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUserWarning\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mNot yet implemented for the test dataset\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m.encode_label()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mProteinDataset.loader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     58\u001b[39m point_cloud_t = torch.from_numpy(point_cloud).to(torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m'\u001b[39m))     \n\u001b[32m     60\u001b[39m point_cloud_t, cls_t = point_cloud_t.type(torch.float32), cls_t.type(torch.float32)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbalancing_factors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.data.append((point_cloud_t, cls_t))\n",
      "\u001b[31mKeyError\u001b[39m: np.int64(10)"
     ]
    }
   ],
   "source": [
    "example_dataframe = create_dataframe(raw_train_dataframe_f1, class_ids=[2, 30, 10], number_of_proteins=100000)\n",
    "example_set_train, example_set_valid = generate_train_valid_set(example_dataframe, transforms, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  5.6880,   9.1380,   5.1850,  ...,  18.7270,  19.7440,  19.8410],\n",
       "         [-20.2360, -20.3460, -20.1250,  ...,  -9.1310, -11.6640, -10.8590],\n",
       "         [-13.3830, -13.4790, -12.5080,  ...,  23.6170,  23.4130,  23.4920]],\n",
       "        device='cuda:0'),\n",
       " tensor(1., device='cuda:0'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_set_train.__getitem__(0, do_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot = example_set_train.__getitem__(0, do_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot type = <class 'tuple'>\n",
      "prot len = 2\n",
      "prot = (tensor([[  5.6880,   9.1380,   5.1850,  ...,  18.7270,  19.7440,  19.8410],\n",
      "        [-20.2360, -20.3460, -20.1250,  ...,  -9.1310, -11.6640, -10.8590],\n",
      "        [-13.3830, -13.4790, -12.5080,  ...,  23.6170,  23.4130,  23.4920]],\n",
      "       device='cuda:0'), tensor(1., device='cuda:0'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{Text(prot, 'prot'):content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot[0] type = <class 'torch.Tensor'>\n",
      "prot[0] device = cuda:0\n",
      "prot[0] dtype = torch.float32\n",
      "prot[0] shape = torch.Size([3, 23402])\n",
      "prot[0] = tensor([[  5.6880,   9.1380,   5.1850,  ...,  18.7270,  19.7440,  19.8410],\n",
      "        [-20.2360, -20.3460, -20.1250,  ...,  -9.1310, -11.6640, -10.8590],\n",
      "        [-13.3830, -13.4790, -12.5080,  ...,  23.6170,  23.4130,  23.4920]],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{Text(prot[0], 'prot[0]'):content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot[0].min() = tensor(-39.9160, device='cuda:0') - prot[0].max() = tensor(38.2570, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f'{prot[0].min() = } - {prot[0].max() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_normalizer = UnitSphereNormalization()\n",
    "prot, _ = example_set_train.__getitem__(0, do_transform=False)\n",
    "prot = torch.transpose(prot, 0, 1)\n",
    "norm_prot = unit_normalizer(prot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_prot type = <class 'torch.Tensor'>\n",
      "norm_prot device = cuda:0\n",
      "norm_prot dtype = torch.float32\n",
      "norm_prot shape = torch.Size([23402, 3])\n",
      "norm_prot = tensor([[ 0.1330, -0.4869, -0.3020],\n",
      "        [ 0.2093, -0.4893, -0.3042],\n",
      "        [ 0.1219, -0.4844, -0.2827],\n",
      "        ...,\n",
      "        [ 0.4213, -0.2413,  0.5160],\n",
      "        [ 0.4438, -0.2973,  0.5115],\n",
      "        [ 0.4460, -0.2795,  0.5132]], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{Text(norm_prot, 'norm_prot'):content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_prot.min() = tensor(-0.9220, device='cuda:0') - norm_prot.max() = tensor(0.8064, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f'{norm_prot.min() = } - {norm_prot.max() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit_normalizer.centroid.min() = tensor(-0.3297, device='cuda:0') - unit_normalizer.centroid.max() = tensor(1.7850, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f'{unit_normalizer.centroid.min() = } - {unit_normalizer.centroid.max() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit_normalizer.centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = prot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.6880, -20.2360, -13.3830], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'symmetria' has no attribute 'decoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msymmetria\u001b[39;00m\n\u001b[32m      3\u001b[39m importlib.reload(symmetria)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m importlib.reload(\u001b[43msymmetria\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoders\u001b[49m)\n\u001b[32m      5\u001b[39m importlib.reload(symmetria.decoders.prediction_head)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'symmetria' has no attribute 'decoders'"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import symmetria\n",
    "importlib.reload(symmetria)\n",
    "importlib.reload(symmetria.decoders)\n",
    "importlib.reload(symmetria.decoders.prediction_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from symmetria.encoders.pointnet_encoder import PointNetEncoder\n",
    "\n",
    "from symmetria.encoders.pointnext.pointnext_encoder_parameters import *\n",
    "from symmetria.encoders.pointnext.pointnext_encoder import PointNeXt\n",
    "\n",
    "from symmetria.decoders.prediction_head import PredictionHead\n",
    "\n",
    "if debug_pointnet:\n",
    "    bs, sz = 1, 2048\n",
    "    encoder = PointNetEncoder(use_bn=False)\n",
    "    mock_x = torch.randn(bs, 3, sz)\n",
    "    output = encoder.forward(mock_x)\n",
    "    print(f'{Text(output, 'output'):inspect}')\n",
    "\n",
    "    decoder = PredictionHead(1024, 96)\n",
    "    output_decoder = decoder.forward(output)\n",
    "    print(f'{Text(output_decoder, 'output_decoder'):inspect}')\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, output_size, max_points, use_bn=False, encoder='pointnet'): # make it prettier like in segmenter\n",
    "        super().__init__()\n",
    "        \n",
    "        if encoder in POINTNEXT_MODEL_CONFIG:\n",
    "            model_cfg = POINTNEXT_MODEL_CONFIG[encoder]     # 'PointNeXt_B' (21.5 M), 'PointNeXt_L2' (32.0 M), 'PointNeXt_XXL' (73.8 M)\n",
    "            self.encoder = PointNeXt(model_cfg)\n",
    "            self.encoder_output_size = output_size          # because of the adapter head (e.g. 2048 -> 1024) within the encoder\n",
    "            print(f\"Using PointNeXt {model_cfg} - encoder: {self.encoder}\")\n",
    "        else:\n",
    "            self.encoder = PointNetEncoder(use_bn)\n",
    "            print(f\"Using PointNet as encoder - encoder: {self.encoder}\")\n",
    "        \n",
    "        self.max_points = max_points\n",
    "\n",
    "        self.input_size = self.get_input_size()        \n",
    "        self.output_size = output_size\n",
    "\n",
    "        print(f'Creating a PredictionHead with {self.input_size} as input and {self.output_size} as output')\n",
    "        self.decoder = PredictionHead(self.input_size, self.output_size, use_bn, use_relu=True)\n",
    "        \n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def get_input_size(self):\n",
    "        mock_x = torch.randn(1, 3, self.max_points)\n",
    "        return self.encoder(mock_x).shape[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        #x = self.softmax(x)\n",
    "\n",
    "        return x #SoftMax already inside the CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a test dataset from ../datasets/train_set_unbalanced-2_cls_0_1.csv\n",
      "Test dataset has 104 rows\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch import optim\n",
    "\n",
    "time_now = datetime.now()\n",
    "time_now = time_now.strftime('%d%m%Y_%H%M%S')\n",
    "_folder = !pwd\n",
    "\n",
    "env = {}\n",
    "\n",
    "### Debug\n",
    "env['debug_loss'] = True\n",
    "\n",
    "env['project_folder'] = _folder[0]\n",
    "env['project'] = 'shrec-2025'\n",
    "env['run'] = env['project'] + '_' + time_now\n",
    "#env['output_dir'] = os.path.join(env['project_folder'], 'wandb', 'run_' + env['run'])\n",
    "if 'output' not in os.listdir(env['project_folder']):\n",
    "    os.mkdir('output')\n",
    "env['output_dir'] = Path(env['project_folder']) / 'output'\n",
    "\n",
    "env['max_points'] = 10000\n",
    "\n",
    "#env['dataset'] = 'train_set-2_cls-1000_images.csv' \n",
    "#env['dataset'] = 'train_set-all.csv'\n",
    "#env['dataset'] = 'train_set-all-cls_except-cls-larger-than-450.csv'\n",
    "#env['dataset'] = 'train_set-symmetria-100k-easy-astroid-citrus-10k-samples.csv'\n",
    "#env['dataset'] = 'train_set-2_cls-1000_images.csv'\n",
    "#env['dataset'] = 'train_set-10_cls-1000_images.csv'\n",
    "env['dataset'] = 'train_set_unbalanced-2_cls_0_1.csv'\n",
    "\n",
    "#env['dataset_path'] = '/mnt/dataset/shrec-2025-protein-classification/v2-20250331'\n",
    "env['dataset_path'] = root\n",
    "env['dataset_balancing'] = False\n",
    "\n",
    "ds_path = Path('../datasets') / env['dataset']\n",
    "print(f'Creating a test dataset from {ds_path}')\n",
    "\n",
    "test = pd.read_csv(ds_path, index_col=0)\n",
    "print(f'Test dataset has {len(test)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PointNet as encoder - encoder: PointNetEncoder(\n",
      "  (input_transform): TNet(\n",
      "    (shared_mlps): Sequential(\n",
      "      (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (3): ReLU()\n",
      "      (4): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (feature_transform): TNet(\n",
      "    (shared_mlps): Sequential(\n",
      "      (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (3): ReLU()\n",
      "      (4): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (5): ReLU()\n",
      "    )\n",
      "    (linear): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=256, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (shared_mlps): Sequential(\n",
      "    (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (shared_mlps_2): Sequential(\n",
      "    (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      ")\n",
      "Creating a PredictionHead with 1024 as input and 2 as output\n"
     ]
    }
   ],
   "source": [
    "env['device'] = 'cuda:0'\n",
    "env['val_pct'] = 0.2\n",
    "env['augmentations_on'] = True\n",
    "'''\n",
    "env['aug'] = [Translate(p=0.8),\n",
    "              UnitSphereNormalization(),\n",
    "              RotateAroundZero(p=0.8),\n",
    "              GaussianNoise(n=10, T=(env['max_points']//2),  p=0.8),\n",
    "              RandomSampler(sample_size=env['max_points'])]\n",
    "'''\n",
    "\n",
    "env['aug'] = [Translate(p=0.5),\n",
    "              UnitSphereNormalization(),\n",
    "              RandomSampler(sample_size=env['max_points']),\n",
    "              GaussianNoise(n=10, T=(env['max_points']//2),  p=0.5),\n",
    "              ]\n",
    "\n",
    "#env['aug'] = [UnitSphereNormalization()]\n",
    "#env['aug'] = []\n",
    "\n",
    "env['bs'] = 16\n",
    "env['epochs'] = 100\n",
    "env['lr'] = 1e-4\n",
    "env['wd'] = 0.05 # because \"PointNeXt is trained with a weight decay of 0.05 for 250 epochs\" - nope, too large, doesn't learn!\n",
    "env['wd'] = 1e-4 # because of this: https://github.com/yanx27/Pointnet_Pointnet2_pytorch/blob/master/train_classification.py\n",
    "\n",
    "env['encoder']   = 'pointnet'\n",
    "#env['encoder']  = 'PointNeXt_XXL'\n",
    "env['model']     = PointNet(output_size=len(test['class_id'].unique()), max_points=env['max_points'], encoder=env['encoder']).to(torch.device(env['device']))\n",
    "\n",
    "env['model_pth'] = '/tmp/pointnet.pth'\n",
    "env['pretrain']  = False\n",
    "\n",
    "if env['pretrain']:\n",
    "    if not Path(env['model_pth']).exists():\n",
    "        !wget 'https://github.com/meder411/PointNet-PyTorch/raw/refs/heads/master/classifier_model_state.pth' --output-document \"{env['model_pth']}\"\n",
    "    else:\n",
    "        print(f'{env['model_pth']} already exists, skipping download...')\n",
    "    torch.load(env['model_pth'], map_location=torch.device(env['device']))\n",
    "\n",
    "env['loss_func'] = nn.CrossEntropyLoss()\n",
    "env['optimizer'] = optim.Adam(env['model'].parameters(), lr=env['lr'], weight_decay=env['wd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'debug_loss': True,\n",
       " 'project_folder': '/home/giorgio/venvs/SHREC/notebooks',\n",
       " 'project': 'shrec-2025',\n",
       " 'run': 'shrec-2025_04042025_110840',\n",
       " 'output_dir': PosixPath('/home/giorgio/venvs/SHREC/notebooks/output'),\n",
       " 'max_points': 10000,\n",
       " 'dataset': 'train_set_unbalanced-2_cls_0_1.csv',\n",
       " 'dataset_path': PosixPath('/mnt/dataset/shrec-2025-protein-classification/v2-20250331'),\n",
       " 'dataset_balancing': False,\n",
       " 'device': 'cuda:0',\n",
       " 'val_pct': 0.2,\n",
       " 'augmentations_on': True,\n",
       " 'aug': [Translate(p=0.5, shift=None, scale=1),\n",
       "  <symmetria.transforms.UnitSphereNormalization.UnitSphereNormalization at 0x751c4c05c0e0>,\n",
       "  <symmetria.transforms.RandomSampler.RandomSampler at 0x751c4c05cf80>,\n",
       "  GaussianNoise(n=10, T=5000, p=0.5)],\n",
       " 'bs': 2,\n",
       " 'epochs': 100,\n",
       " 'lr': 0.0001,\n",
       " 'wd': 0.0001,\n",
       " 'encoder': 'pointnet',\n",
       " 'model': PointNet(\n",
       "   (encoder): PointNetEncoder(\n",
       "     (input_transform): TNet(\n",
       "       (shared_mlps): Sequential(\n",
       "         (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "         (1): ReLU()\n",
       "         (2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "         (3): ReLU()\n",
       "         (4): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "         (5): ReLU()\n",
       "       )\n",
       "       (linear): Sequential(\n",
       "         (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "         (3): ReLU()\n",
       "         (4): Linear(in_features=256, out_features=9, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (feature_transform): TNet(\n",
       "       (shared_mlps): Sequential(\n",
       "         (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "         (1): ReLU()\n",
       "         (2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "         (3): ReLU()\n",
       "         (4): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "         (5): ReLU()\n",
       "       )\n",
       "       (linear): Sequential(\n",
       "         (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "         (3): ReLU()\n",
       "         (4): Linear(in_features=256, out_features=4096, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (shared_mlps): Sequential(\n",
       "       (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "       (1): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (shared_mlps_2): Sequential(\n",
       "       (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "       (1): LeakyReLU(negative_slope=0.01)\n",
       "       (2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "       (3): LeakyReLU(negative_slope=0.01)\n",
       "       (4): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "     )\n",
       "   )\n",
       "   (decoder): PredictionHead(\n",
       "     (decoder_head): Sequential(\n",
       "       (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "       (1): ReLU()\n",
       "       (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "       (3): ReLU()\n",
       "       (4): Linear(in_features=256, out_features=2, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'model_pth': '/tmp/pointnet.pth',\n",
       " 'pretrain': False,\n",
       " 'loss_func': CrossEntropyLoss(),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0.0001\n",
       " )}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_loss(logits, target, loss_func):\n",
    "    output    = torch.argmax(logits, dim=1)\n",
    "    target    = target.to(torch.int64)\n",
    "    #softmaxed = torch.softmax(logits, dim=1)\n",
    "    loss      = loss_func(logits, target)\n",
    "    return loss, output, target\n",
    "\n",
    "def process_confusion_matrix(matrix):\n",
    "    weights = np.zeros(len(matrix))\n",
    "    cls_acc = np.zeros(len(matrix))\n",
    "    for i in range(len(matrix)):\n",
    "        row_sum = 0\n",
    "        for j in range(len(matrix)):\n",
    "            row_sum += matrix[i][j]\n",
    "        \n",
    "        weights[i] = row_sum\n",
    "        cls_acc[i] = matrix[i][i]/row_sum\n",
    "\n",
    "    total_acc = np.sum(cls_acc * weights)/np.sum(weights)\n",
    "\n",
    "    return total_acc, cls_acc\n",
    "\n",
    "def train(train_dl, valid_dl, env):\n",
    "    net = env['model']\n",
    "    loss_func = env['loss_func']\n",
    "    optimizer = env['optimizer']\n",
    "\n",
    "    wandb.init(project=env['project'], name=env['run'], dir=env['output_dir'], config=env)\n",
    "\n",
    "    for epoch in range(env['epochs']):\n",
    "        print('/' * 20 + f' Epoch: {epoch + 1} ' + '/' * 20)\n",
    "        for step, batch in enumerate(tqdm(train_dl)):\n",
    "            input_p, target = batch\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            logits = net(input_p)\n",
    "            #print(f'{Text(output, 'output'):content}')\n",
    "            #print(f'{Text(target, 'target'):content}')\n",
    "            \n",
    "            '''\n",
    "            output = torch.argmax(logits, dim=1)\n",
    "            target = target.to(torch.int64)\n",
    "            loss = loss_func(output, target)\n",
    "            '''\n",
    "            loss, output, target = calculate_loss(logits, target, loss_func)\n",
    "            if env['debug_loss'] and step % 1000 == 0:\n",
    "                print(f'pred: {int(output)} - target: {int(target)} - loss: {loss:.3f} - raw pred: {logits}')\n",
    "            wandb.log({'epoch': epoch,\n",
    "                    'loss': loss})\n",
    "            #print(f'Training loss: {loss}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('/' * 20 + ' Validation ' + '/' * 20)\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            acc = []\n",
    "            confusion_matrix = None\n",
    "            for step, batch in enumerate(tqdm(valid_dl)):\n",
    "                input_p, target = batch\n",
    "                \n",
    "                logits = net(input_p)\n",
    "\n",
    "                '''\n",
    "                output = torch.argmax(logits, dim=1)\n",
    "                target = target.to(torch.int64)\n",
    "                loss = loss_func(output, target).cpu()\n",
    "                '''\n",
    "                loss, output, target = calculate_loss(logits, target, loss_func)\n",
    "                if env['debug_loss'] and step % 1000 == 0:\n",
    "                    print(f'pred: {int(output)} - target: {int(target)} - loss: {loss:.3f} - raw pred: {logits}')\n",
    "                \n",
    "                val_loss.append(loss.cpu())\n",
    "\n",
    "                # Mean accuracy\n",
    "                #matching = [torch.argmax(i) == torch.argmax(j) for i, j in zip(output, target)]\n",
    "                matching = [int(i) == int(j) for i, j in zip(output, target)]\n",
    "                accuracy = matching.count(True)/len(matching)\n",
    "\n",
    "                if env['debug_loss'] and step % 10 == 0:\n",
    "                    print(f'{matching = } - {accuracy = }')\n",
    "                acc.append(accuracy)\n",
    "\n",
    "                confusion_matrix_batch = confusion_matrix(target.cpu().numpy(), output.cpu().numpy())\n",
    "                if confusion_matrix is None:\n",
    "                    confusion_matrix = confusion_matrix_batch\n",
    "                else:\n",
    "                    confusion_matrix += confusion_matrix_batch\n",
    "            \n",
    "            total_acc, cls_acc = process_confusion_matrix(confusion_matrix)\n",
    "\n",
    "            log = {'epoch': epoch,\n",
    "                   'step': step,\n",
    "                   'val_loss': np.mean(val_loss),\n",
    "                   'mean_acc': np.mean(acc),\n",
    "                   'total_acc': total_acc}\n",
    "            \n",
    "            for cls, val in enumerate(cls_acc):\n",
    "                key = f'class_{cls}_acc'\n",
    "                log[key] = val\n",
    "            \n",
    "            wandb.log(log)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83/83 [00:00<00:00, 145.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 164.92it/s]\n"
     ]
    }
   ],
   "source": [
    "test_train_ds, test_valid_ds = generate_train_valid_set(df=test,\n",
    "                                                        tfms=env['aug'],\n",
    "                                                        val_pct=env['val_pct'],\n",
    "                                                        root=mountpoint,\n",
    "                                                        balancing=env['dataset_balancing'])\n",
    "\n",
    "test_train_dl, test_valid_dl = DataLoader(test_train_ds, batch_size=env['bs'], shuffle=True), DataLoader(test_valid_ds, batch_size=env['bs'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:00<00:00, 149.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[0.82178218 0.66666667]\n",
      "0.8125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ds = ProteinDataset(test, env['aug'], root=mountpoint)\n",
    "dl = DataLoader(ds, batch_size=16, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    acc_mean = []\n",
    "    conf_m = None\n",
    "    for batch in dl:\n",
    "        input, target = batch\n",
    "\n",
    "        logits = env['model'](input)    \n",
    "        loss, output, target = calculate_loss(logits, target, env['loss_func'])\n",
    "        \n",
    "        matching = [int(i) == int(j) for i, j in zip(output, target)]\n",
    "        accuracy = matching.count(True)/len(matching)\n",
    "        acc_mean.append(accuracy)\n",
    "    \n",
    "        conf_m_batch = confusion_matrix(target.cpu().numpy(), output.cpu().numpy())\n",
    "        if conf_m is None:\n",
    "            conf_m = conf_m_batch\n",
    "        else:\n",
    "            conf_m += conf_m_batch       \n",
    "\n",
    "total_acc, cls_acc = process_confusion_matrix(conf_m)\n",
    "\n",
    "print(total_acc)\n",
    "print(cls_acc)\n",
    "\n",
    "print(np.mean(acc_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(test_train_dl, test_valid_dl, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmount_ramdrive(mountpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHREC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
