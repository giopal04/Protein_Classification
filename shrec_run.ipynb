{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointNet to Classify Proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8ugd_8:R:3U_model1.vtk', '8h0v_18:R:c_model1.vtk', '3j3q_1:DX:4F_model1.vtk', '4u4u_23:XC:d1_model1.vtk', '6rny_4:H:H_model1.vtk']\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9244 entries, 0 to 9243\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   protein_id        9244 non-null   object\n",
      " 1   class_id          9244 non-null   int64 \n",
      " 2   number_of_points  9244 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 288.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>number_of_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ugd_8:R:3U_model1</td>\n",
       "      <td>96</td>\n",
       "      <td>5916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8h0v_18:R:c_model1</td>\n",
       "      <td>86</td>\n",
       "      <td>10078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3j3q_1:DX:4F_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>18432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4u4u_23:XC:d1_model1</td>\n",
       "      <td>83</td>\n",
       "      <td>8242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6rny_4:H:H_model1</td>\n",
       "      <td>34</td>\n",
       "      <td>9204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9239</th>\n",
       "      <td>3j3y_1:HL:6R_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>18342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9240</th>\n",
       "      <td>4u4y_15:O:C3_model1</td>\n",
       "      <td>91</td>\n",
       "      <td>12976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9241</th>\n",
       "      <td>7w31_26:AA:c_model1</td>\n",
       "      <td>18</td>\n",
       "      <td>16454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>3j4k_1:E:E_model1</td>\n",
       "      <td>90</td>\n",
       "      <td>23188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9243</th>\n",
       "      <td>3j3y_1:UIA:fE_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>17888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9244 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                protein_id  class_id  number_of_points\n",
       "0       8ugd_8:R:3U_model1        96              5916\n",
       "1       8h0v_18:R:c_model1        86             10078\n",
       "2      3j3q_1:DX:4F_model1         8             18432\n",
       "3     4u4u_23:XC:d1_model1        83              8242\n",
       "4        6rny_4:H:H_model1        34              9204\n",
       "...                    ...       ...               ...\n",
       "9239   3j3y_1:HL:6R_model1         8             18342\n",
       "9240   4u4y_15:O:C3_model1        91             12976\n",
       "9241   7w31_26:AA:c_model1        18             16454\n",
       "9242     3j4k_1:E:E_model1        90             23188\n",
       "9243  3j3y_1:UIA:fE_model1         8             17888\n",
       "\n",
       "[9244 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "from datasetStudy import *\n",
    "\n",
    "# Multiple function here depends on thi variable\n",
    "root = '/mnt/dataset/shrec-2025-protein-classification/v2-20250331' \n",
    "\n",
    "train_data = os.listdir(os.path.join(root, 'train'))\n",
    "train_data_cls = pd.read_csv('datasets/train_set-all.csv', sep=',', index_col=0)\n",
    "\n",
    "print(train_data[:5])\n",
    "print()\n",
    "print(train_data_cls.info())\n",
    "train_data_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>number_of_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [protein_id, class_id, number_of_points]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_disconnected_mesh(train_data_cls, 96, False)\n",
    "\n",
    "disconnected_dict = {}\n",
    "for idx in range(97):\n",
    "    disconnected_dict[idx] = possible_disconnected_mesh(train_data_cls, idx)\n",
    "\n",
    "#print(disconnected_dict)\n",
    "\n",
    "_, damage = possible_disconnected_mesh(train_data_cls, 8, True); damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes which have between 0 and 5 element: 16/97\n"
     ]
    }
   ],
   "source": [
    "dist = cls_distribution(train_data_cls)\n",
    "inspect_distribution(dist, l_lim=0, u_lim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls 0: 101\t101\t101\t0\t0\t101\t\n",
      "cls 1: 3\t3\t0\t0\t0\t0\t\n",
      "cls 2: 17\t17\t17\t17\t17\t0\t\n",
      "cls 3: 21\t21\t21\t21\t21\t0\t\n",
      "cls 4: 15\t15\t15\t15\t15\t0\t\n",
      "cls 5: 127\t127\t127\t0\t0\t127\t\n",
      "cls 6: 25\t25\t25\t25\t25\t0\t\n",
      "cls 7: 75\t75\t75\t75\t0\t0\t\n",
      "cls 8: 2054\t2054\t2054\t0\t0\t2054\t\n",
      "cls 9: 66\t66\t66\t66\t0\t0\t\n",
      "cls 10: 14\t14\t14\t14\t14\t0\t\n",
      "cls 11: 43\t43\t43\t43\t43\t0\t\n",
      "cls 12: 10\t10\t10\t10\t10\t0\t\n",
      "cls 13: 22\t22\t22\t22\t22\t0\t\n",
      "cls 14: 489\t489\t489\t0\t0\t489\t\n",
      "cls 15: 89\t89\t89\t89\t0\t0\t\n",
      "cls 16: 154\t154\t154\t0\t0\t154\t\n",
      "cls 17: 116\t116\t116\t0\t0\t116\t\n",
      "cls 18: 76\t76\t76\t76\t0\t0\t\n",
      "cls 19: 42\t42\t42\t42\t42\t0\t\n",
      "cls 20: 7\t7\t0\t0\t0\t0\t\n",
      "cls 21: 74\t74\t74\t74\t0\t0\t\n",
      "cls 22: 58\t58\t58\t58\t0\t0\t\n",
      "cls 23: 10\t10\t10\t10\t10\t0\t\n",
      "cls 24: 10\t10\t10\t10\t10\t0\t\n",
      "cls 25: 143\t143\t143\t0\t0\t143\t\n",
      "cls 26: 2\t2\t0\t0\t0\t0\t\n",
      "cls 27: 3\t3\t0\t0\t0\t0\t\n",
      "cls 28: 18\t18\t18\t18\t18\t0\t\n",
      "cls 29: 14\t14\t14\t14\t14\t0\t\n",
      "cls 30: 3\t3\t0\t0\t0\t0\t\n",
      "cls 31: 33\t33\t33\t33\t33\t0\t\n",
      "cls 32: 93\t93\t93\t93\t0\t0\t\n",
      "cls 33: 126\t126\t126\t0\t0\t126\t\n",
      "cls 34: 73\t73\t73\t73\t0\t0\t\n",
      "cls 35: 23\t23\t23\t23\t23\t0\t\n",
      "cls 36: 8\t8\t0\t0\t0\t0\t\n",
      "cls 37: 119\t119\t119\t0\t0\t119\t\n",
      "cls 38: 34\t34\t34\t34\t34\t0\t\n",
      "cls 39: 19\t19\t19\t19\t19\t0\t\n",
      "cls 40: 95\t95\t95\t95\t0\t0\t\n",
      "cls 41: 95\t95\t95\t95\t0\t0\t\n",
      "cls 42: 2\t2\t0\t0\t0\t0\t\n",
      "cls 43: 70\t70\t70\t70\t0\t0\t\n",
      "cls 44: 2\t2\t0\t0\t0\t0\t\n",
      "cls 45: 109\t109\t109\t0\t0\t109\t\n",
      "cls 46: 44\t44\t44\t44\t44\t0\t\n",
      "cls 47: 37\t37\t37\t37\t37\t0\t\n",
      "cls 48: 27\t27\t27\t27\t27\t0\t\n",
      "cls 49: 49\t49\t49\t49\t49\t0\t\n",
      "cls 50: 2\t2\t0\t0\t0\t0\t\n",
      "cls 51: 71\t71\t71\t71\t0\t0\t\n",
      "cls 52: 76\t76\t76\t76\t0\t0\t\n",
      "cls 53: 53\t53\t53\t53\t0\t0\t\n",
      "cls 54: 128\t128\t128\t0\t0\t128\t\n",
      "cls 55: 26\t26\t26\t26\t26\t0\t\n",
      "cls 56: 654\t654\t654\t0\t0\t654\t\n",
      "cls 57: 9\t9\t0\t0\t0\t0\t\n",
      "cls 58: 2\t2\t0\t0\t0\t0\t\n",
      "cls 59: 54\t54\t54\t54\t0\t0\t\n",
      "cls 60: 73\t73\t73\t73\t0\t0\t\n",
      "cls 61: 240\t240\t240\t0\t0\t240\t\n",
      "cls 62: 135\t135\t135\t0\t0\t135\t\n",
      "cls 63: 2\t2\t0\t0\t0\t0\t\n",
      "cls 64: 57\t57\t57\t57\t0\t0\t\n",
      "cls 65: 10\t10\t10\t10\t10\t0\t\n",
      "cls 66: 54\t54\t54\t54\t0\t0\t\n",
      "cls 67: 17\t17\t17\t17\t17\t0\t\n",
      "cls 68: 5\t5\t0\t0\t0\t0\t\n",
      "cls 69: 80\t80\t80\t80\t0\t0\t\n",
      "cls 70: 223\t223\t223\t0\t0\t223\t\n",
      "cls 71: 102\t102\t102\t0\t0\t102\t\n",
      "cls 72: 2\t2\t0\t0\t0\t0\t\n",
      "cls 73: 6\t6\t0\t0\t0\t0\t\n",
      "cls 74: 150\t150\t150\t0\t0\t150\t\n",
      "cls 75: 69\t69\t69\t69\t0\t0\t\n",
      "cls 76: 10\t10\t10\t10\t10\t0\t\n",
      "cls 77: 2\t2\t0\t0\t0\t0\t\n",
      "cls 78: 5\t5\t0\t0\t0\t0\t\n",
      "cls 79: 78\t78\t78\t78\t0\t0\t\n",
      "cls 80: 57\t57\t57\t57\t0\t0\t\n",
      "cls 81: 58\t58\t58\t58\t0\t0\t\n",
      "cls 82: 2\t2\t0\t0\t0\t0\t\n",
      "cls 83: 115\t115\t115\t0\t0\t115\t\n",
      "cls 84: 18\t18\t18\t18\t18\t0\t\n",
      "cls 85: 49\t49\t49\t49\t49\t0\t\n",
      "cls 86: 203\t203\t203\t0\t0\t203\t\n",
      "cls 87: 103\t103\t103\t0\t0\t103\t\n",
      "cls 88: 97\t97\t97\t97\t0\t0\t\n",
      "cls 89: 3\t3\t0\t0\t0\t0\t\n",
      "cls 90: 788\t788\t788\t0\t0\t788\t\n",
      "cls 91: 118\t118\t118\t0\t0\t118\t\n",
      "cls 92: 129\t129\t129\t0\t0\t129\t\n",
      "cls 93: 61\t61\t61\t61\t0\t0\t\n",
      "cls 94: 52\t52\t52\t52\t0\t0\t\n",
      "cls 95: 2\t2\t0\t0\t0\t0\t\n",
      "cls 96: 35\t35\t35\t35\t35\t0\t\n",
      "len(train_data_cls) = 9244\n",
      "len(train_data_f1) = 9244\n",
      "len(train_data_f2) = 9172\n",
      "len(train_data_f3) = 2546\n",
      "len(train_data_f4) = 692\n",
      "len(train_data_f5) = 6626\n"
     ]
    }
   ],
   "source": [
    "dist_all = cls_distribution(train_data_cls)\n",
    "\n",
    "raw_train_dataframe_f1 = number_of_point_filter(train_data_cls, 1000)\n",
    "dist_f1 = cls_distribution(raw_train_dataframe_f1)\n",
    "\n",
    "raw_train_dataframe_f2 = number_of_class_filter(raw_train_dataframe_f1, 10)\n",
    "dist_f2 = cls_distribution(raw_train_dataframe_f2)\n",
    "\n",
    "raw_train_dataframe_f3 = number_of_class_filter(raw_train_dataframe_f1, 10, 100)\n",
    "dist_f3 = cls_distribution(raw_train_dataframe_f3)\n",
    "\n",
    "raw_train_dataframe_f4 = number_of_class_filter(raw_train_dataframe_f1, 10, 50)\n",
    "dist_f4 = cls_distribution(raw_train_dataframe_f4)\n",
    "\n",
    "raw_train_dataframe_f5 = number_of_class_filter(raw_train_dataframe_f1, 100)\n",
    "dist_f5 = cls_distribution(raw_train_dataframe_f5)\n",
    "\n",
    "distributions = [dist_all, dist_f1, dist_f2, dist_f3, dist_f4, dist_f5]\n",
    "for idx in range(len(dist_all)):\n",
    "    output = f'cls {idx}: '\n",
    "    for dist in distributions:\n",
    "        output += f'{print_dist(dist, idx)}\\t'\n",
    "    print(output)\n",
    "\n",
    "print(f'{len(train_data_cls) = }')\n",
    "print(f'{len(raw_train_dataframe_f1) = }')\n",
    "print(f'{len(raw_train_dataframe_f2) = }')\n",
    "print(f'{len(raw_train_dataframe_f3) = }')\n",
    "print(f'{len(raw_train_dataframe_f4) = }')\n",
    "print(f'{len(raw_train_dataframe_f5) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_selected = [8, 90, 56, 14, 61, 70, 86, 5, 16, 25, 54, 62, 74, 83, 91, 92, 17, 45, 87, 71]\n",
    "\n",
    "df_3 = create_dataframe(raw_train_dataframe_f1, cls_selected[:2], 500)\n",
    "df_4 = create_dataframe(raw_train_dataframe_f1, cls_selected[:10], 100)\n",
    "df_5 = create_dataframe(raw_train_dataframe_f1, cls_selected[:20], 50)\n",
    "df_6 = create_dataframe(raw_train_dataframe_f1, cls_selected[:20], 100)\n",
    "\n",
    "'''\n",
    "df_3.to_csv('datasets/train_set-2_cls-1000_images.csv')\n",
    "df_4.to_csv('datasets/train_set-10_cls-1000_images.csv')\n",
    "df_5.to_csv('datasets/train_set-20_cls-1000_images.csv')\n",
    "df_6.to_csv('datasets/train_set-20_cls-2000_images.csv')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format import Text\n",
    "import torch\n",
    "from symmetria.transformations import *\n",
    "\n",
    "#NOTE: not used\n",
    "#NOTE: also useless \n",
    "\n",
    "class BenchmarkShape():\n",
    "    def __init__(self, points):\n",
    "        self.points = points # Tensor with shape [3, N]\n",
    "    \n",
    "    def apply_rotation(self, rot):\n",
    "        ones = torch.ones((1,self.points.shape[1]))\n",
    "        self.coords = torch.concatenate((self.points, ones))\n",
    "        \n",
    "        assert rot.device == self.coords.device, \"rotation matrix and points on different devices\"\n",
    "\n",
    "        self.coords = rot@self.coords\n",
    "        \n",
    "        self.points = self.coords[:3,:]\n",
    "\n",
    "    def apply_traslation(self, x, y, z):\n",
    "        print(self.points.shape)\n",
    "        self.points[0] = self.points[0] + x\n",
    "        self.points[1] = self.points[1] + y\n",
    "        self.points[2] = self.points[2] + z\n",
    "    \n",
    "    #Applies uniform noise. n is a parameter for the amount of noise. T is the number of points to apply\n",
    "    def apply_uniform_noise(self, n, T):\n",
    "        num_points = self.points.shape[1]\n",
    "\n",
    "        indices = torch.randperm(num_points, device=self.points.device)[:T]\n",
    "    \n",
    "        self.points[:,indices] = self.points[:, indices] + (2*torch.rand(3, T, device=self.points.device)-1)/n\n",
    "    \n",
    "    def apply_gaussian_noise(self, n, T):\n",
    "        num_points = self.points.shape[1]\n",
    "\n",
    "        indices = torch.randperm(num_points, device=self.points.device)[:T]\n",
    "\n",
    "        self.points[:, indices] = self.points[:, indices] + torch.rand(3, T, device=self.points.device)/n\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rectangle = np.array([[0, 2, 0, 0, 2, 2, 0, 2],\n",
    "                             [0, 0, 1, 0, 1, 0, 1, 1],\n",
    "                             [0, 0, 0, 1, 0, 1, 1, 1]]).astype(np.float32)\n",
    "\n",
    "points_rectangle_t = torch.from_numpy(points_rectangle)\n",
    "\n",
    "rot = random_rotation_matrix(rotate_x=False, rotate_z=False)\n",
    "\n",
    "shapebench = BenchmarkShape(points_rectangle_t)\n",
    "#shapebench.apply_rotation(torch.from_numpy(rot))\n",
    "#shapebench.apply_traslation(1,1,0)\n",
    "#shapebench.apply_uniform_noise(100, 1)\n",
    "#shapebench.apply_gaussian_noise(100, 1)\n",
    "\n",
    "rect = pv.PolyData(shapebench.points.numpy().T)\n",
    "#rect.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class RotateAroundZero():\n",
    "    def __init__(self, p=0.5, rot=None):\n",
    "        self.p = p\n",
    "        self.rot = rot\n",
    "\n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            if self.rot == None:\n",
    "                self.rot = torch.from_numpy(random_rotation_matrix())\n",
    "        \n",
    "            self.rot = self.rot.to(torch.device(points.device))\n",
    "\n",
    "            ones = torch.ones((1,points.shape[0]), device=points.device)\n",
    "            coords = torch.concatenate((torch.transpose(points, 0, 1), ones))\n",
    "            \n",
    "            coords = self.rot@coords\n",
    "            \n",
    "            points = torch.transpose(coords[:3,:], 0, 1)\n",
    "\n",
    "        return points\n",
    "    \n",
    "class Translate():\n",
    "    def __init__(self, p=0.5, shift=None, scale=1):\n",
    "        self.p = p\n",
    "        self.shift = shift\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, points):        \n",
    "        if random() < self.p:\n",
    "            if self.shift == None:\n",
    "                self.shift = (torch.rand(1, device=points.device) * self.scale,\n",
    "                              torch.rand(1, device=points.device) * self.scale,\n",
    "                              torch.rand(1, device=points.device) * self.scale)\n",
    "            \n",
    "            for i in range(len(self.shift)):\n",
    "                points[:,i] += self.shift[i]\n",
    "                   \n",
    "        return points\n",
    "    \n",
    "class UniformNoise():\n",
    "    def __init__(self, n, T, p=0.5):\n",
    "        self.p = p\n",
    "        self.n, self.T = n, T\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            num_points = points.shape[0]\n",
    "\n",
    "            indices = torch.randperm(num_points, device=points.device)[:self.T]\n",
    "\n",
    "            points[indices,:] = points[indices,:] + (2*torch.rand(self.T, 3, device=points.device)-1)/self.n\n",
    "        \n",
    "        return points\n",
    "    \n",
    "class GaussianNoise():\n",
    "    def __init__(self, n, T, p=0.5):\n",
    "        self.p = p\n",
    "        self.n, self.T = n, T\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            num_points = points.shape[0]\n",
    "\n",
    "            indices = torch.randperm(num_points, device=points.device)[:self.T]\n",
    "    \n",
    "            points[indices,:] = points[indices,:] + torch.rand(self.T, 3, device=points.device)/self.n\n",
    "\n",
    "        return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rectangle = np.array([[0, 2, 0, 0, 2, 2, 0, 2],\n",
    "                             [0, 0, 1, 0, 1, 0, 1, 1],\n",
    "                             [0, 0, 0, 1, 0, 1, 1, 1]]).astype(np.float32).T\n",
    "\n",
    "points_rectangle_t = torch.from_numpy(points_rectangle).to(torch.device('cuda:0'))\n",
    "\n",
    "translate = UniformNoise(n=10, T=8, p=0.8)\n",
    "points_rectangle_t = translate(points_rectangle_t)\n",
    "\n",
    "rect = pv.PolyData(points_rectangle)\n",
    "#rect.plot()\n",
    "\n",
    "rect_tfm = pv.PolyData(points_rectangle_t.cpu().numpy())\n",
    "#rect_tfm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_train_valid_set_boring(df, val_pct, seed=42):\\n    train_size = int(len(df) * val_pct)    \\n    df_train = df.sample(train_size, random_state=seed)\\n    df_valid = df.drop(df_train.index)\\n\\n    return df_train, df_valid\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lzma\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from symmetria.transforms.RandomSampler import RandomSampler\n",
    "from symmetria.transforms.UnitSphereNormalization import UnitSphereNormalization\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_df, tfms, root='/mnt/dataset/shrec-2025-protein-classification/v2-20250331', extention='xz', train=True):\n",
    "        super().__init__()\n",
    "        self.df = data_df\n",
    "        self.tfms = tfms\n",
    "        self.extention = extention\n",
    "        \n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            if self.extention == 'vtk':\n",
    "                self.root = os.path.join(root, 'train')\n",
    "\n",
    "            elif self.extention == 'xz':\n",
    "                self.root = os.path.join(root, 'train-xz')\n",
    "            \n",
    "            else:\n",
    "                raise UserWarning('Extention not supported')\n",
    "            \n",
    "        else:\n",
    "            raise UserWarning('Not yet implemented for the test dataset')\n",
    "        \n",
    "        self.encode_label()\n",
    "        self.loader()\n",
    "    \n",
    "    def loader(self):\n",
    "        self.data = []\n",
    "        \n",
    "        for index in tqdm(self.df.index):\n",
    "            protein, cls, nop = self.df['protein_id'].loc[index], self.df['class_id'].loc[index], self.df['number_of_points'].loc[index]\n",
    "\n",
    "            cls_t = torch.tensor(self.encoded_cls[cls]).to(torch.device('cuda:0'))\n",
    "\n",
    "            if self.extention == 'vtk':\n",
    "                point_cloud = self.get_vtk_points(protein)\n",
    "\n",
    "            elif self.extention == 'xz':\n",
    "                point_cloud = self.get_xz_points(protein, cls, nop)\n",
    "\n",
    "            point_cloud_t = torch.from_numpy(point_cloud).to(torch.device('cuda:0'))     \n",
    "\n",
    "            point_cloud_t, cls_t = point_cloud_t.type(torch.float32), cls_t.type(torch.float32)\n",
    "            self.data.append((point_cloud_t, cls_t))\n",
    "\n",
    "    def get_vtk_points(self, name):\n",
    "        prot_file = name + '.' + self.extention\n",
    "        prot_file = os.path.join(self.root, prot_file)\n",
    "        \n",
    "        prot_mesh = pv.read(prot_file)\n",
    "\n",
    "        return prot_mesh.points\n",
    "\n",
    "    def get_xz_points(self, name, cls, nop):\n",
    "        cls, nop = str(cls), str(nop)\n",
    "\n",
    "        while len(cls) < 2:\n",
    "            cls = '0' + cls\n",
    "        \n",
    "        while len(nop) < 6:\n",
    "            nop = '0' + nop\n",
    "\n",
    "        prot_file = cls + '-' + nop +  '-' + name.replace(':', '+') + '.' + self.extention\n",
    "        prot_file = os.path.join(self.root, prot_file)        \n",
    "        \n",
    "        with lzma.open(prot_file, 'rt') as f:\n",
    "            point_cloud = np.loadtxt(f)\n",
    "\n",
    "        return point_cloud\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index, raw=False):\n",
    "        prot, cls = self.data[index]\n",
    "        \n",
    "        if not raw:\n",
    "            for tfm in self.tfms:\n",
    "                prot = tfm(prot)\n",
    "\n",
    "        prot = torch.transpose(prot, 0, 1)\n",
    "        return (prot, cls)\n",
    "    \n",
    "    def encode_label(self):\n",
    "        self.encoded_cls = {}\n",
    "        \n",
    "        prot_clss = self.df['class_id'].unique()\n",
    "        for idx, cls in enumerate(prot_clss):\n",
    "            self.encoded_cls[cls] = np.eye(len(prot_clss))[idx]\n",
    "\n",
    "    def render_pointcloud(self, index):\n",
    "        prot, _ = self.data[index]\n",
    "        prot = torch.transpose(prot, 0, 1).numpy()\n",
    "        cloud = pv.PolyData(prot)\n",
    "        print(cloud)\n",
    "        cloud.plot()\n",
    "    \n",
    "transforms = [Translate(p=0.8, scale=5),\n",
    "              UnitSphereNormalization(),\n",
    "              RotateAroundZero(p=0.8),\n",
    "              GaussianNoise(n=10, T=2500, p=0.8),\n",
    "              RandomSampler(sample_size=5000)]\n",
    "\n",
    "def generate_train_valid_set(df, tfms, val_pct, seed=42, **kwargs):\n",
    "    '''\n",
    "    Can also take some other arguments to be passed to the dataset initializer\n",
    "\n",
    "            -> path (str): path to the parent directory containing the train files \n",
    "    '''\n",
    "\n",
    "    train_size = int(len(df) * (1 - val_pct))    \n",
    "    df_train = df.sample(train_size, random_state=seed)\n",
    "    df_valid = df.drop(df_train.index)\n",
    "\n",
    "    return ProteinDataset(df_train, tfms, **kwargs), ProteinDataset(df_valid, tfms, **kwargs)\n",
    "\n",
    "'''\n",
    "def generate_train_valid_set_boring(df, val_pct, seed=42):\n",
    "    train_size = int(len(df) * val_pct)    \n",
    "    df_train = df.sample(train_size, random_state=seed)\n",
    "    df_valid = df.drop(df_train.index)\n",
    "\n",
    "    return df_train, df_valid\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1643/1643 [00:29<00:00, 55.47it/s]\n",
      "100%|██████████| 411/411 [00:07<00:00, 55.16it/s]\n"
     ]
    }
   ],
   "source": [
    "example_set = raw_train_dataframe_f1[raw_train_dataframe_f1['class_id'] == 8]\n",
    "example_set_train, example_set_valid = generate_train_valid_set(example_set, transforms, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4708, -0.4053, -0.5113],\n",
       "        [ 0.4930, -0.4078, -0.5076],\n",
       "        [ 0.4110, -0.4043, -0.4881],\n",
       "        ...,\n",
       "        [ 0.2582,  0.4253,  0.5525],\n",
       "        [ 0.2795,  0.3104,  0.5509],\n",
       "        [ 0.2830,  0.3629,  0.5580]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = UnitSphereNormalization()\n",
    "prot, _ = example_set_train.__getitem__(0, raw=True)\n",
    "sampler(prot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PointNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type = <class 'torch.Tensor'>\n",
      "output device = cpu\n",
      "output dtype = torch.float32\n",
      "output shape = torch.Size([1, 1024])\n",
      "\n",
      "output_decoder type = <class 'torch.Tensor'>\n",
      "output_decoder device = cpu\n",
      "output_decoder dtype = torch.float32\n",
      "output_decoder shape = torch.Size([1, 96])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from symmetria.encoders.pointnet_encoder import PointNetEncoder\n",
    "from symmetria.decoders.prediction_head import PredictionHead\n",
    "\n",
    "bs, sz = 1, 2048\n",
    "encoder = PointNetEncoder(use_bn=False)\n",
    "mock_x = torch.randn(bs, 3, sz)\n",
    "output = encoder.forward(mock_x)\n",
    "print(f'{Text(output, 'output'):inspect}')\n",
    "\n",
    "\n",
    "decoder = PredictionHead(1024, 96)\n",
    "output_decoder = decoder.forward(output)\n",
    "print(f'{Text(output_decoder, 'output_decoder'):inspect}')\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, output_size, use_bn=False): # make it prettier like in segmenter\n",
    "        super().__init__()\n",
    "        self.encoder = PointNetEncoder(use_bn)\n",
    "\n",
    "        self.input_size = self.get_input_size()        \n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.decoder = PredictionHead(self.input_size, self.output_size, use_bn)\n",
    "\n",
    "    def get_input_size(self):\n",
    "        mock_x = torch.randn(1, 3, 1024)\n",
    "        return self.encoder(mock_x).shape[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x #SoftMax already inside the CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch import optim\n",
    "\n",
    "time_now = datetime.now()\n",
    "time_now = time_now.strftime('%d%m%Y_%H%M%S')\n",
    "_folder = !pwd\n",
    "\n",
    "env = {}\n",
    "env['project_folder'] = _folder[0]\n",
    "env['project'] = 'shrec-2025'\n",
    "env['run'] = env['project'] + '_' + time_now\n",
    "env['output_dir'] = os.path.join(env['project_folder'], 'wandb', 'run_' + env['run'])\n",
    "\n",
    "env['dataset'] = 'train_set-2_cls-1000_images.csv' \n",
    "env['dataset_path'] = '/mnt/dataset/shrec-2025-protein-classification/v2-20250331'\n",
    "env['device'] = 'cuda:0'\n",
    "env['val_pct'] = 0.2\n",
    "env['augmentations_on'] = True\n",
    "env['aug'] = [Translate(p=0.8),\n",
    "              UnitSphereNormalization(),\n",
    "              RotateAroundZero(p=0.8),\n",
    "              GaussianNoise(n=10, T=2500,  p=0.8),\n",
    "              RandomSampler(sample_size=5000)]\n",
    "env['bs'] = 4\n",
    "\n",
    "env['epochs'] = 20\n",
    "env['lr'] = 1e-3\n",
    "\n",
    "env['model'] = PointNet(len(test['class_id'].unique())).to(torch.device(env['device']))\n",
    "env['loss_func'] = nn.CrossEntropyLoss()\n",
    "env['optimizer'] = optim.Adam(env['model'].parameters(), lr=env['lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train(train_dl, valid_dl, env):\n",
    "    net = env['model']\n",
    "    loss_func = env['loss_func']\n",
    "    optimizer = env['optimizer']\n",
    "\n",
    "    wandb.init(project=env['project'], name=env['run'], dir=env['output_dir'], config=env)\n",
    "\n",
    "    for epoch in range(env['epochs']):\n",
    "        #print('/' * 20 + f' Epoch: {epoch + 1} ' + '/' * 20)\n",
    "        for step, batch in enumerate(tqdm(train_dl)):\n",
    "            input_p, target = batch\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            output = net(input_p)\n",
    "            #print(f'{Text(output, 'output'):content}')\n",
    "            #print(f'{Text(target, 'target'):content}')\n",
    "            \n",
    "            loss = loss_func(output, target)\n",
    "            wandb.log({'epoch': epoch,\n",
    "                    'loss': loss})\n",
    "            #print(f'Training loss: {loss}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #print('/' * 20 + ' Validation ' + '/' * 20)\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            acc = []\n",
    "            for step, batch in enumerate(tqdm(valid_dl)):\n",
    "                input_p, target = batch\n",
    "                \n",
    "                output = net(input_p)\n",
    "\n",
    "                loss = loss_func(output, target).cpu()\n",
    "                val_loss.append(loss)\n",
    "\n",
    "                matching = [torch.argmax(i) == torch.argmax(j) for i, j in zip(output, target)]\n",
    "                accuracy = matching.count(True)/len(matching)\n",
    "                acc.append(accuracy)\n",
    "            wandb.log({'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'val_loss': np.mean(val_loss),\n",
    "                    'accuracy': np.mean(acc)})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:13<00:00, 58.72it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 61.60it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(os.path.join('datasets', env['dataset']), index_col=0)\n",
    "\n",
    "test_train_ds, test_valid_ds = generate_train_valid_set(test, env['aug'], env['val_pct'])\n",
    "test_train_dl, test_valid_dl = DataLoader(test_train_ds, batch_size=env['bs'], shuffle=True), DataLoader(test_valid_ds, batch_size=env['bs'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /home/giorgio/venvs/SHREC/wandb/run_shrec-2025_01042025_143341/wandb/ wasn't writable, using system temp directory.\n",
      "100%|██████████| 200/200 [00:08<00:00, 22.97it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 146.93it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 87.45it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 192.85it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.77it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 193.40it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.47it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 195.34it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.11it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 191.79it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 85.14it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 191.38it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 85.91it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 194.85it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 84.73it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 194.68it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.01it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 192.33it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.23it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 188.48it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 85.48it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 190.96it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.32it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 192.09it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.48it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 195.49it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.59it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 190.92it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 85.89it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 194.34it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 85.97it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 195.07it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 87.70it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 196.54it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.62it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 191.02it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 87.03it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 192.96it/s]\n",
      "100%|██████████| 200/200 [00:02<00:00, 86.45it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 192.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▅█▇▄▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>loss</td><td>▇▆▇▇▅▃▂▄▂▁▂▅▁▃▁▁▂▁▄▁▂▁▃▁▁▂▁▁▁▁▁█▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▁▁▁▂▂▂▄▄▄▄▃▄▅▅▅▄▄█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.02</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>loss</td><td>0.01186</td></tr><tr><td>step</td><td>49</td></tr><tr><td>val_loss</td><td>9.75798</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">shrec-2025_01042025_143147</strong> at: <a href='https://wandb.ai/g-palmieri4998-cnr-imati/shrec-2025/runs/7xnlqlsv' target=\"_blank\">https://wandb.ai/g-palmieri4998-cnr-imati/shrec-2025/runs/7xnlqlsv</a><br> View project at: <a href='https://wandb.ai/g-palmieri4998-cnr-imati/shrec-2025' target=\"_blank\">https://wandb.ai/g-palmieri4998-cnr-imati/shrec-2025</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20250401_143210-7xnlqlsv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(test_train_dl, test_valid_dl, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHREC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
