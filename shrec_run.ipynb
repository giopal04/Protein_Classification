{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointNet to Classify Proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8ugd_8:R:3U_model1.vtk', '8h0v_18:R:c_model1.vtk', '3j3q_1:DX:4F_model1.vtk', '4u4u_23:XC:d1_model1.vtk', '6rny_4:H:H_model1.vtk']\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9244 entries, 0 to 9243\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   protein_id        9244 non-null   object\n",
      " 1   class_id          9244 non-null   int64 \n",
      " 2   number_of_points  9244 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 288.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>number_of_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ugd_8:R:3U_model1</td>\n",
       "      <td>96</td>\n",
       "      <td>5916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8h0v_18:R:c_model1</td>\n",
       "      <td>86</td>\n",
       "      <td>10078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3j3q_1:DX:4F_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>18432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4u4u_23:XC:d1_model1</td>\n",
       "      <td>83</td>\n",
       "      <td>8242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6rny_4:H:H_model1</td>\n",
       "      <td>34</td>\n",
       "      <td>9204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9239</th>\n",
       "      <td>3j3y_1:HL:6R_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>18342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9240</th>\n",
       "      <td>4u4y_15:O:C3_model1</td>\n",
       "      <td>91</td>\n",
       "      <td>12976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9241</th>\n",
       "      <td>7w31_26:AA:c_model1</td>\n",
       "      <td>18</td>\n",
       "      <td>16454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9242</th>\n",
       "      <td>3j4k_1:E:E_model1</td>\n",
       "      <td>90</td>\n",
       "      <td>23188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9243</th>\n",
       "      <td>3j3y_1:UIA:fE_model1</td>\n",
       "      <td>8</td>\n",
       "      <td>17888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9244 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                protein_id  class_id  number_of_points\n",
       "0       8ugd_8:R:3U_model1        96              5916\n",
       "1       8h0v_18:R:c_model1        86             10078\n",
       "2      3j3q_1:DX:4F_model1         8             18432\n",
       "3     4u4u_23:XC:d1_model1        83              8242\n",
       "4        6rny_4:H:H_model1        34              9204\n",
       "...                    ...       ...               ...\n",
       "9239   3j3y_1:HL:6R_model1         8             18342\n",
       "9240   4u4y_15:O:C3_model1        91             12976\n",
       "9241   7w31_26:AA:c_model1        18             16454\n",
       "9242     3j4k_1:E:E_model1        90             23188\n",
       "9243  3j3y_1:UIA:fE_model1         8             17888\n",
       "\n",
       "[9244 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "from datasetStudy import *\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "debug_pointnet = False\n",
    "\n",
    "# Multiple function here depends on thi variable\n",
    "#root = '/mnt/dataset/shrec-2025-protein-classification/v2-20250331' \n",
    "base_path = Path('/mnt/')\n",
    "#base_path = Path('/mnt')\n",
    "root = base_path / 'dataset/shrec-2025-protein-classification/v2-20250331' \n",
    "\n",
    "#train_data = os.listdir(os.path.join(root, 'train'))\n",
    "#train_data_cls = pd.read_csv('datasets/train_set-all.csv', sep=',', index_col=0)\n",
    "\n",
    "train_data = os.listdir(root / 'train')\n",
    "train_data_cls = pd.read_csv('./datasets/train_set-all.csv', sep=',', index_col=0)\n",
    "\n",
    "print(train_data[:5])\n",
    "print()\n",
    "print(train_data_cls.info())\n",
    "train_data_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>number_of_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [protein_id, class_id, number_of_points]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_disconnected_mesh(train_data_cls, 96, False)\n",
    "\n",
    "disconnected_dict = {}\n",
    "for idx in range(97):\n",
    "    disconnected_dict[idx] = possible_disconnected_mesh(train_data_cls, idx)\n",
    "\n",
    "#print(disconnected_dict)\n",
    "\n",
    "_, damaged = possible_disconnected_mesh(train_data_cls, 8, True); damaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes which have between 0 and 5 element: 16/97\n"
     ]
    }
   ],
   "source": [
    "dist = cls_distribution(train_data_cls)\n",
    "inspect_distribution(dist, l_lim=0, u_lim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls 0: 101\t101\t101\t0\t0\t101\t\n",
      "cls 1: 3\t3\t0\t0\t0\t0\t\n",
      "cls 2: 17\t17\t17\t17\t17\t0\t\n",
      "cls 3: 21\t21\t21\t21\t21\t0\t\n",
      "cls 4: 15\t15\t15\t15\t15\t0\t\n",
      "cls 5: 127\t127\t127\t0\t0\t127\t\n",
      "cls 6: 25\t25\t25\t25\t25\t0\t\n",
      "cls 7: 75\t75\t75\t75\t0\t0\t\n",
      "cls 8: 2054\t2054\t2054\t0\t0\t2054\t\n",
      "cls 9: 66\t66\t66\t66\t0\t0\t\n",
      "cls 10: 14\t14\t14\t14\t14\t0\t\n",
      "cls 11: 43\t43\t43\t43\t43\t0\t\n",
      "cls 12: 10\t10\t10\t10\t10\t0\t\n",
      "cls 13: 22\t22\t22\t22\t22\t0\t\n",
      "cls 14: 489\t489\t489\t0\t0\t489\t\n",
      "cls 15: 89\t89\t89\t89\t0\t0\t\n",
      "cls 16: 154\t154\t154\t0\t0\t154\t\n",
      "cls 17: 116\t116\t116\t0\t0\t116\t\n",
      "cls 18: 76\t76\t76\t76\t0\t0\t\n",
      "cls 19: 42\t42\t42\t42\t42\t0\t\n",
      "cls 20: 7\t7\t0\t0\t0\t0\t\n",
      "cls 21: 74\t74\t74\t74\t0\t0\t\n",
      "cls 22: 58\t58\t58\t58\t0\t0\t\n",
      "cls 23: 10\t10\t10\t10\t10\t0\t\n",
      "cls 24: 10\t10\t10\t10\t10\t0\t\n",
      "cls 25: 143\t143\t143\t0\t0\t143\t\n",
      "cls 26: 2\t2\t0\t0\t0\t0\t\n",
      "cls 27: 3\t3\t0\t0\t0\t0\t\n",
      "cls 28: 18\t18\t18\t18\t18\t0\t\n",
      "cls 29: 14\t14\t14\t14\t14\t0\t\n",
      "cls 30: 3\t3\t0\t0\t0\t0\t\n",
      "cls 31: 33\t33\t33\t33\t33\t0\t\n",
      "cls 32: 93\t93\t93\t93\t0\t0\t\n",
      "cls 33: 126\t126\t126\t0\t0\t126\t\n",
      "cls 34: 73\t73\t73\t73\t0\t0\t\n",
      "cls 35: 23\t23\t23\t23\t23\t0\t\n",
      "cls 36: 8\t8\t0\t0\t0\t0\t\n",
      "cls 37: 119\t119\t119\t0\t0\t119\t\n",
      "cls 38: 34\t34\t34\t34\t34\t0\t\n",
      "cls 39: 19\t19\t19\t19\t19\t0\t\n",
      "cls 40: 95\t95\t95\t95\t0\t0\t\n",
      "cls 41: 95\t95\t95\t95\t0\t0\t\n",
      "cls 42: 2\t2\t0\t0\t0\t0\t\n",
      "cls 43: 70\t70\t70\t70\t0\t0\t\n",
      "cls 44: 2\t2\t0\t0\t0\t0\t\n",
      "cls 45: 109\t109\t109\t0\t0\t109\t\n",
      "cls 46: 44\t44\t44\t44\t44\t0\t\n",
      "cls 47: 37\t37\t37\t37\t37\t0\t\n",
      "cls 48: 27\t27\t27\t27\t27\t0\t\n",
      "cls 49: 49\t49\t49\t49\t49\t0\t\n",
      "cls 50: 2\t2\t0\t0\t0\t0\t\n",
      "cls 51: 71\t71\t71\t71\t0\t0\t\n",
      "cls 52: 76\t76\t76\t76\t0\t0\t\n",
      "cls 53: 53\t53\t53\t53\t0\t0\t\n",
      "cls 54: 128\t128\t128\t0\t0\t128\t\n",
      "cls 55: 26\t26\t26\t26\t26\t0\t\n",
      "cls 56: 654\t654\t654\t0\t0\t654\t\n",
      "cls 57: 9\t9\t0\t0\t0\t0\t\n",
      "cls 58: 2\t2\t0\t0\t0\t0\t\n",
      "cls 59: 54\t54\t54\t54\t0\t0\t\n",
      "cls 60: 73\t73\t73\t73\t0\t0\t\n",
      "cls 61: 240\t240\t240\t0\t0\t240\t\n",
      "cls 62: 135\t135\t135\t0\t0\t135\t\n",
      "cls 63: 2\t2\t0\t0\t0\t0\t\n",
      "cls 64: 57\t57\t57\t57\t0\t0\t\n",
      "cls 65: 10\t10\t10\t10\t10\t0\t\n",
      "cls 66: 54\t54\t54\t54\t0\t0\t\n",
      "cls 67: 17\t17\t17\t17\t17\t0\t\n",
      "cls 68: 5\t5\t0\t0\t0\t0\t\n",
      "cls 69: 80\t80\t80\t80\t0\t0\t\n",
      "cls 70: 223\t223\t223\t0\t0\t223\t\n",
      "cls 71: 102\t102\t102\t0\t0\t102\t\n",
      "cls 72: 2\t2\t0\t0\t0\t0\t\n",
      "cls 73: 6\t6\t0\t0\t0\t0\t\n",
      "cls 74: 150\t150\t150\t0\t0\t150\t\n",
      "cls 75: 69\t69\t69\t69\t0\t0\t\n",
      "cls 76: 10\t10\t10\t10\t10\t0\t\n",
      "cls 77: 2\t2\t0\t0\t0\t0\t\n",
      "cls 78: 5\t5\t0\t0\t0\t0\t\n",
      "cls 79: 78\t78\t78\t78\t0\t0\t\n",
      "cls 80: 57\t57\t57\t57\t0\t0\t\n",
      "cls 81: 58\t58\t58\t58\t0\t0\t\n",
      "cls 82: 2\t2\t0\t0\t0\t0\t\n",
      "cls 83: 115\t115\t115\t0\t0\t115\t\n",
      "cls 84: 18\t18\t18\t18\t18\t0\t\n",
      "cls 85: 49\t49\t49\t49\t49\t0\t\n",
      "cls 86: 203\t203\t203\t0\t0\t203\t\n",
      "cls 87: 103\t103\t103\t0\t0\t103\t\n",
      "cls 88: 97\t97\t97\t97\t0\t0\t\n",
      "cls 89: 3\t3\t0\t0\t0\t0\t\n",
      "cls 90: 788\t788\t788\t0\t0\t788\t\n",
      "cls 91: 118\t118\t118\t0\t0\t118\t\n",
      "cls 92: 129\t129\t129\t0\t0\t129\t\n",
      "cls 93: 61\t61\t61\t61\t0\t0\t\n",
      "cls 94: 52\t52\t52\t52\t0\t0\t\n",
      "cls 95: 2\t2\t0\t0\t0\t0\t\n",
      "cls 96: 35\t35\t35\t35\t35\t0\t\n",
      "len(train_data_cls) = 9244\n",
      "len(raw_train_dataframe_f1) = 9244\n",
      "len(raw_train_dataframe_f2) = 9172\n",
      "len(raw_train_dataframe_f3) = 2546\n",
      "len(raw_train_dataframe_f4) = 692\n",
      "len(raw_train_dataframe_f5) = 6626\n"
     ]
    }
   ],
   "source": [
    "dist_all = cls_distribution(train_data_cls)\n",
    "\n",
    "raw_train_dataframe_f1 = number_of_point_filter(train_data_cls, 1000)\n",
    "dist_f1 = cls_distribution(raw_train_dataframe_f1)\n",
    "\n",
    "raw_train_dataframe_f2 = number_of_class_filter(raw_train_dataframe_f1, 10)\n",
    "dist_f2 = cls_distribution(raw_train_dataframe_f2)\n",
    "\n",
    "raw_train_dataframe_f3 = number_of_class_filter(raw_train_dataframe_f1, 10, 100)\n",
    "dist_f3 = cls_distribution(raw_train_dataframe_f3)\n",
    "\n",
    "raw_train_dataframe_f4 = number_of_class_filter(raw_train_dataframe_f1, 10, 50)\n",
    "dist_f4 = cls_distribution(raw_train_dataframe_f4)\n",
    "\n",
    "raw_train_dataframe_f5 = number_of_class_filter(raw_train_dataframe_f1, 100)\n",
    "dist_f5 = cls_distribution(raw_train_dataframe_f5)\n",
    "\n",
    "distributions = [dist_all, dist_f1, dist_f2, dist_f3, dist_f4, dist_f5]\n",
    "for idx in range(len(dist_all)):\n",
    "    output = f'cls {idx}: '\n",
    "    for dist in distributions:\n",
    "        output += f'{print_dist(dist, idx)}\\t'\n",
    "    print(output)\n",
    "\n",
    "print(f'{len(train_data_cls) = }')\n",
    "print(f'{len(raw_train_dataframe_f1) = }')\n",
    "print(f'{len(raw_train_dataframe_f2) = }')\n",
    "print(f'{len(raw_train_dataframe_f3) = }')\n",
    "print(f'{len(raw_train_dataframe_f4) = }')\n",
    "print(f'{len(raw_train_dataframe_f5) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf_3.to_csv('datasets/train_set-2_cls-1000_images.csv')\\ndf_4.to_csv('datasets/train_set-10_cls-1000_images.csv')\\ndf_5.to_csv('datasets/train_set-20_cls-1000_images.csv')\\ndf_6.to_csv('datasets/train_set-20_cls-2000_images.csv')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_selected = [8, 90, 56, 14, 61, 70, 86, 5, 16, 25, 54, 62, 74, 83, 91, 92, 17, 45, 87, 71]\n",
    "\n",
    "df_3 = create_dataframe(raw_train_dataframe_f1, cls_selected[:2], 500)\n",
    "df_4 = create_dataframe(raw_train_dataframe_f1, cls_selected[:10], 100)\n",
    "df_5 = create_dataframe(raw_train_dataframe_f1, cls_selected[:20], 50)\n",
    "df_6 = create_dataframe(raw_train_dataframe_f1, cls_selected[:20], 100)\n",
    "\n",
    "'''\n",
    "df_3.to_csv('datasets/train_set-2_cls-1000_images.csv')\n",
    "df_4.to_csv('datasets/train_set-10_cls-1000_images.csv')\n",
    "df_5.to_csv('datasets/train_set-20_cls-1000_images.csv')\n",
    "df_6.to_csv('datasets/train_set-20_cls-2000_images.csv')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format import Text\n",
    "import torch\n",
    "from symmetria.transformations import *\n",
    "from symmetria.shapes import BenchmarkShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_rectangle = np.array([[0, 2, 0, 0, 2, 2, 0, 2],\n",
    "                             [0, 0, 1, 0, 1, 0, 1, 1],\n",
    "                             [0, 0, 0, 1, 0, 1, 1, 1]]).astype(np.float32)\n",
    "\n",
    "points_rectangle_t = torch.from_numpy(points_rectangle)\n",
    "\n",
    "rot = random_rotation_matrix(rotate_x=False, rotate_z=False)\n",
    "\n",
    "shapebench = BenchmarkShape(points_rectangle_t)\n",
    "#shapebench.apply_rotation(torch.from_numpy(rot))\n",
    "#shapebench.apply_traslation(1,1,0)\n",
    "#shapebench.apply_uniform_noise(100, 1)\n",
    "#shapebench.apply_gaussian_noise(100, 1)\n",
    "\n",
    "rect = pv.PolyData(shapebench.points.numpy().T)\n",
    "#rect.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "class RotateAroundZero():\n",
    "    def __init__(self, p=0.5, rot=None):\n",
    "        self.p = p\n",
    "        self.rot = rot\n",
    "\n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            if self.rot == None:\n",
    "                rot = torch.from_numpy(random_rotation_matrix())\n",
    "            else:\n",
    "                rot = self.rot\n",
    "        \n",
    "            rot = rot.to(torch.device(points.device))\n",
    "\n",
    "            ones = torch.ones((1,points.shape[0]), device=points.device)\n",
    "            coords = torch.concatenate((torch.transpose(points, 0, 1), ones))\n",
    "            \n",
    "            coords = rot@coords\n",
    "            \n",
    "            points = torch.transpose(coords[:3,:], 0, 1)\n",
    "\n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'RotateAroundZero(p={repr(self.p)}, rot={repr(self.rot)})'\n",
    "    \n",
    "class Translate():\n",
    "    def __init__(self, p=0.5, shift=None, scale=1):\n",
    "        self.p = p\n",
    "        self.shift = shift\n",
    "        self.scale = scale\n",
    "\n",
    "    def __call__(self, points):        \n",
    "        if random() < self.p:\n",
    "            if self.shift == None:\n",
    "                shift = (torch.rand(1, device=points.device) * self.scale,\n",
    "                         torch.rand(1, device=points.device) * self.scale,\n",
    "                         torch.rand(1, device=points.device) * self.scale)\n",
    "            else:\n",
    "                shift = self.shift\n",
    "            \n",
    "            for i in range(len(shift)):\n",
    "                points[:,i] += shift[i]\n",
    "                   \n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Translate(p={repr(self.p)}, shift={repr(self.shift)}, scale={repr(self.scale)})'\n",
    "    \n",
    "class UniformNoise():\n",
    "    def __init__(self, n, T, p=0.5):\n",
    "        self.p = p\n",
    "        self.n, self.T = n, T\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            num_points = points.shape[0]\n",
    "\n",
    "            indices = torch.randperm(num_points, device=points.device)[:self.T]\n",
    "\n",
    "            points[indices,:] = points[indices,:] + (2*torch.rand(self.T, 3, device=points.device)-1)/self.n\n",
    "        \n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'UniformNoise(n={repr(self.n)}, T={repr(self.T)}, p={repr(self.p)})'\n",
    "    \n",
    "class GaussianNoise():\n",
    "    def __init__(self, n, T, p=0.5):\n",
    "        self.p = p\n",
    "        self.n, self.T = n, T\n",
    "    \n",
    "    def __call__(self, points):\n",
    "        if random() < self.p:\n",
    "            num_points = points.shape[0]\n",
    "\n",
    "            indices = torch.randperm(num_points, device=points.device)[:self.T]\n",
    "    \n",
    "            points[indices,:] = points[indices,:] + torch.rand(self.T, 3, device=points.device)/self.n\n",
    "\n",
    "        return points\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'GaussianNoise(n={repr(self.n)}, T={repr(self.T)}, p={repr(self.p)})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate(p=1, shift=None, scale=1)\n",
      "Translate(p=1, shift=None, scale=1)\n"
     ]
    }
   ],
   "source": [
    "points_rectangle = np.array([[0, 2, 0, 0, 2, 2, 0, 2],\n",
    "                             [0, 0, 1, 0, 1, 0, 1, 1],\n",
    "                             [0, 0, 0, 1, 0, 1, 1, 1]]).astype(np.float32).T\n",
    "\n",
    "points_rectangle_t = torch.from_numpy(points_rectangle).to(torch.device('cuda:0'))\n",
    "\n",
    "translate = Translate(p=1)\n",
    "print(translate)\n",
    "points_rectangle_t = translate(points_rectangle_t)\n",
    "print(translate)\n",
    "rect = pv.PolyData(points_rectangle)\n",
    "#rect.plot()\n",
    "\n",
    "rect_tfm = pv.PolyData(points_rectangle_t.cpu().numpy())\n",
    "#rect_tfm.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef generate_train_valid_set_boring(df, val_pct, seed=42):\\n    train_size = int(len(df) * val_pct)    \\n    df_train = df.sample(train_size, random_state=seed)\\n    df_valid = df.drop(df_train.index)\\n\\n    return df_train, df_valid\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lzma\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from symmetria.transforms.RandomSampler import RandomSampler\n",
    "from symmetria.transforms.UnitSphereNormalization import UnitSphereNormalization\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, data_df, tfms, root='/mnt/dataset/shrec-2025-protein-classification/v2-20250331', extention='xz', train=True):\n",
    "        super().__init__()\n",
    "        self.df = data_df\n",
    "        self.tfms = tfms\n",
    "        self.extention = extention\n",
    "        \n",
    "        self.train = train\n",
    "        if self.train:\n",
    "            if self.extention == 'vtk':\n",
    "                self.root = os.path.join(root, 'train')\n",
    "\n",
    "            elif self.extention == 'xz':\n",
    "                self.root = os.path.join(root, 'train-xz')\n",
    "            \n",
    "            else:\n",
    "                raise UserWarning('Extention not supported')\n",
    "            \n",
    "        else:\n",
    "            raise UserWarning('Not yet implemented for the test dataset')\n",
    "        \n",
    "        self.encode_label()\n",
    "        self.loader()\n",
    "    \n",
    "    def loader(self):\n",
    "        self.data = []\n",
    "        \n",
    "        for index in tqdm(self.df.index):\n",
    "            protein, cls, nop = self.df['protein_id'].loc[index], self.df['class_id'].loc[index], self.df['number_of_points'].loc[index]\n",
    "\n",
    "            cls_t = torch.tensor(self.encoded_cls[cls]).to(torch.device('cuda:0'))\n",
    "\n",
    "            if self.extention == 'vtk':\n",
    "                point_cloud = self.get_vtk_points(protein)\n",
    "\n",
    "            elif self.extention == 'xz':\n",
    "                point_cloud = self.get_xz_points(protein, cls, nop)\n",
    "\n",
    "            point_cloud_t = torch.from_numpy(point_cloud).to(torch.device('cuda:0'))     \n",
    "\n",
    "            point_cloud_t, cls_t = point_cloud_t.type(torch.float32), cls_t.type(torch.float32)\n",
    "            self.data.append((point_cloud_t, cls_t))\n",
    "\n",
    "    def get_vtk_points(self, name):\n",
    "        prot_file = name + '.' + self.extention\n",
    "        prot_file = os.path.join(self.root, prot_file)\n",
    "        \n",
    "        prot_mesh = pv.read(prot_file)\n",
    "\n",
    "        return prot_mesh.points\n",
    "\n",
    "    def get_xz_points(self, name, cls, nop):\n",
    "        cls, nop = str(cls), str(nop)\n",
    "\n",
    "        while len(cls) < 2:\n",
    "            cls = '0' + cls\n",
    "        \n",
    "        while len(nop) < 6:\n",
    "            nop = '0' + nop\n",
    "\n",
    "        prot_file = cls + '-' + nop +  '-' + name.replace(':', '+') + '.' + self.extention\n",
    "        prot_file = os.path.join(self.root, prot_file)        \n",
    "        \n",
    "        with lzma.open(prot_file, 'rt') as f:\n",
    "            point_cloud = np.loadtxt(f)\n",
    "\n",
    "        return point_cloud\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index, raw=False):\n",
    "        prot, cls = self.data[index]\n",
    "        \n",
    "        if not raw:\n",
    "            for tfm in self.tfms:\n",
    "                prot = tfm(prot)\n",
    "\n",
    "        prot = torch.transpose(prot, 0, 1)\n",
    "        return (prot, cls)\n",
    "    \n",
    "    def encode_label(self):\n",
    "        self.encoded_cls = {}\n",
    "        \n",
    "        prot_clss = np.sort(self.df['class_id'].unique())\n",
    "        \n",
    "        #One hot encoding\n",
    "        '''\n",
    "        for idx, cls in enumerate(prot_clss):\n",
    "            self.encoded_cls[cls] = np.eye(len(prot_clss))[idx]\n",
    "        '''\n",
    "\n",
    "        for idx, cls in enumerate(prot_clss):\n",
    "            self.encoded_cls[cls] = int(idx)\n",
    "\n",
    "    def render_pointcloud(self, index):\n",
    "        prot, _ = self.data[index]\n",
    "        prot = torch.transpose(prot, 0, 1).numpy()\n",
    "        cloud = pv.PolyData(prot)\n",
    "        print(cloud)\n",
    "        cloud.plot()\n",
    "    \n",
    "\n",
    "transforms = [UnitSphereNormalization(),\n",
    "              Translate(p=0.8, scale=1),\n",
    "              RotateAroundZero(p=0.8),\n",
    "              GaussianNoise(n=100, T=2500, p=0.8),\n",
    "              RandomSampler(sample_size=5000)]\n",
    "\n",
    "\n",
    "def generate_train_valid_set(df, tfms, val_pct, seed=42, **kwargs):\n",
    "    '''\n",
    "    Can also take some other arguments to be passed to the dataset initializer\n",
    "\n",
    "            -> path (str): path to the parent directory containing the train files \n",
    "    '''\n",
    "\n",
    "    train_size = int(len(df) * (1 - val_pct))    \n",
    "    df_train = df.sample(train_size, random_state=seed)\n",
    "    df_valid = df.drop(df_train.index)\n",
    "\n",
    "    return ProteinDataset(df_train, tfms, root=root, **kwargs), ProteinDataset(df_valid, tfms, root=root, **kwargs)\n",
    "\n",
    "'''\n",
    "def generate_train_valid_set_boring(df, val_pct, seed=42):\n",
    "    train_size = int(len(df) * val_pct)    \n",
    "    df_train = df.sample(train_size, random_state=seed)\n",
    "    df_valid = df.drop(df_train.index)\n",
    "\n",
    "    return df_train, df_valid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all data for the class 2\n",
      "Using all data for the class 30\n",
      "Using all data for the class 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 41.61it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 56.23it/s]\n"
     ]
    }
   ],
   "source": [
    "example_dataframe = create_dataframe(raw_train_dataframe_f1, [2, 30, 10])\n",
    "example_set_train, example_set_valid = generate_train_valid_set(example_dataframe, transforms, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(2., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n",
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "example_set_train.__getitem__(0, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 18730])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = example_set_train.__getitem__(0, raw=True)\n",
    "item[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item[0].min() = tensor(-42.3550, device='cuda:0') - item[0].max() = tensor(41.5100, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f'{item[0].min() = } - {item[0].max() = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0122,  0.0124,  0.0113,  ..., -0.0026, -0.0016, -0.0019],\n",
       "        [-0.0048, -0.0050, -0.0046,  ...,  0.0003, -0.0013, -0.0007],\n",
       "        [-0.0073, -0.0074, -0.0067,  ...,  0.0023,  0.0029,  0.0026]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = UnitSphereNormalization()\n",
    "prot, _ = example_set_train.__getitem__(0, raw=True)\n",
    "sampler(prot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from symmetria.encoders.pointnet_encoder import PointNetEncoder\n",
    "from symmetria.decoders.prediction_head import PredictionHead\n",
    "\n",
    "if debug_pointnet:\n",
    "    bs, sz = 1, 2048\n",
    "    encoder = PointNetEncoder(use_bn=False)\n",
    "    mock_x = torch.randn(bs, 3, sz)\n",
    "    output = encoder.forward(mock_x)\n",
    "    print(f'{Text(output, 'output'):inspect}')\n",
    "\n",
    "    decoder = PredictionHead(1024, 96)\n",
    "    output_decoder = decoder.forward(output)\n",
    "    print(f'{Text(output_decoder, 'output_decoder'):inspect}')\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, output_size, max_points, use_bn=False): # make it prettier like in segmenter\n",
    "        super().__init__()\n",
    "        self.encoder = PointNetEncoder(use_bn)\n",
    "        self.max_points = max_points\n",
    "\n",
    "        self.input_size = self.get_input_size()        \n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.decoder = PredictionHead(self.input_size, self.output_size, use_bn)\n",
    "\n",
    "    def get_input_size(self):\n",
    "        mock_x = torch.randn(1, 3, self.max_points)\n",
    "        return self.encoder(mock_x).shape[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x #SoftMax already inside the CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a test dataset from datasets/train_set-all.csv\n",
      "Test dataset has 9244 rows\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch import optim\n",
    "\n",
    "time_now = datetime.now()\n",
    "time_now = time_now.strftime('%d%m%Y_%H%M%S')\n",
    "_folder = !pwd\n",
    "\n",
    "env = {}\n",
    "env['project_folder'] = _folder[0]\n",
    "env['project'] = 'shrec-2025'\n",
    "env['run'] = env['project'] + '_' + time_now\n",
    "#env['output_dir'] = os.path.join(env['project_folder'], 'wandb', 'run_' + env['run'])\n",
    "if 'output' not in os.listdir(env['project_folder']):\n",
    "    os.mkdir('output')\n",
    "env['output_dir'] = Path(env['project_folder']) / 'output'\n",
    "\n",
    "env['max_points'] = 4000\n",
    "\n",
    "#env['dataset'] = 'train_set-2_cls-1000_images.csv' \n",
    "env['dataset'] = 'train_set-all.csv' \n",
    "#env['dataset_path'] = '/mnt/dataset/shrec-2025-protein-classification/v2-20250331'\n",
    "env['dataset_path'] = root\n",
    "\n",
    "ds_path = Path('./datasets') / env['dataset']\n",
    "print(f'Creating a test dataset from {ds_path}')\n",
    "\n",
    "test = pd.read_csv(ds_path, index_col=0)\n",
    "print(f'Test dataset has {len(test)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env['device'] = 'cuda:0'\n",
    "env['val_pct'] = 0.2\n",
    "env['augmentations_on'] = True\n",
    "env['aug'] = [Translate(p=0.8),\n",
    "              UnitSphereNormalization(),\n",
    "              RotateAroundZero(p=0.8),\n",
    "              GaussianNoise(n=10, T=(env['max_points']//2),  p=0.8),\n",
    "              RandomSampler(sample_size=env['max_points'])]\n",
    "\n",
    "env['bs'] = 4\n",
    "env['epochs'] = 20\n",
    "env['lr'] = 1e-3\n",
    "\n",
    "env['model'] = PointNet(output_size=len(test['class_id'].unique()), max_points=env['max_points']).to(torch.device(env['device']))\n",
    "env['loss_func'] = nn.CrossEntropyLoss()\n",
    "env['optimizer'] = optim.Adam(env['model'].parameters(), lr=env['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project_folder': '/home/giorgio/venvs/SHREC',\n",
       " 'project': 'shrec-2025',\n",
       " 'run': 'shrec-2025_01042025_161836',\n",
       " 'output_dir': PosixPath('/home/giorgio/venvs/SHREC/wandb/shrec-2025_01042025_161836'),\n",
       " 'max_points': 4000,\n",
       " 'dataset': 'train_set-all.csv',\n",
       " 'dataset_path': PosixPath('/mnt/dataset/shrec-2025-protein-classification/v2-20250331'),\n",
       " 'device': 'cuda:0',\n",
       " 'val_pct': 0.2,\n",
       " 'augmentations_on': True,\n",
       " 'aug': [<__main__.Translate at 0x7f98ac532060>,\n",
       "  <symmetria.transforms.UnitSphereNormalization.UnitSphereNormalization at 0x7f98a6b63b60>,\n",
       "  <__main__.RotateAroundZero at 0x7f98a410fec0>,\n",
       "  <__main__.GaussianNoise at 0x7f98a410fe90>,\n",
       "  <symmetria.transforms.RandomSampler.RandomSampler at 0x7f993e368530>],\n",
       " 'bs': 4,\n",
       " 'epochs': 20,\n",
       " 'lr': 0.001,\n",
       " 'model': PointNet(\n",
       "   (encoder): PointNetEncoder(\n",
       "     (input_transform): TNet(\n",
       "       (shared_mlps): Sequential(\n",
       "         (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "         (1): ReLU()\n",
       "         (2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "         (3): ReLU()\n",
       "         (4): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "         (5): ReLU()\n",
       "       )\n",
       "       (linear): Sequential(\n",
       "         (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "         (3): ReLU()\n",
       "         (4): Linear(in_features=256, out_features=9, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (feature_transform): TNet(\n",
       "       (shared_mlps): Sequential(\n",
       "         (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "         (1): ReLU()\n",
       "         (2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "         (3): ReLU()\n",
       "         (4): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "         (5): ReLU()\n",
       "       )\n",
       "       (linear): Sequential(\n",
       "         (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "         (1): ReLU()\n",
       "         (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "         (3): ReLU()\n",
       "         (4): Linear(in_features=256, out_features=4096, bias=True)\n",
       "       )\n",
       "     )\n",
       "     (shared_mlps): Sequential(\n",
       "       (0): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "       (1): LeakyReLU(negative_slope=0.01)\n",
       "     )\n",
       "     (shared_mlps_2): Sequential(\n",
       "       (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "       (1): LeakyReLU(negative_slope=0.01)\n",
       "       (2): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "       (3): LeakyReLU(negative_slope=0.01)\n",
       "       (4): Conv1d(256, 1024, kernel_size=(1,), stride=(1,))\n",
       "     )\n",
       "   )\n",
       "   (decoder): PredictionHead(\n",
       "     (decoder_head): Sequential(\n",
       "       (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "       (1): LeakyReLU(negative_slope=0.01)\n",
       "       (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "       (3): LeakyReLU(negative_slope=0.01)\n",
       "       (4): Linear(in_features=256, out_features=97, bias=True)\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " 'loss_func': CrossEntropyLoss(),\n",
       " 'optimizer': Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.001\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " )}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def train(train_dl, valid_dl, env):\n",
    "    net = env['model']\n",
    "    loss_func = env['loss_func']\n",
    "    optimizer = env['optimizer']\n",
    "\n",
    "    wandb.init(project=env['project'], name=env['run'], dir=env['output_dir'], config=env)\n",
    "\n",
    "    for epoch in range(env['epochs']):\n",
    "        print('/' * 20 + f' Epoch: {epoch + 1} ' + '/' * 20)\n",
    "        for step, batch in enumerate(tqdm(train_dl)):\n",
    "            input_p, target = batch\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            output = net(input_p)\n",
    "            #print(f'{Text(output, 'output'):content}')\n",
    "            #print(f'{Text(target, 'target'):content}')\n",
    "            \n",
    "            loss = loss_func(output, target)\n",
    "            wandb.log({'epoch': epoch,\n",
    "                    'loss': loss})\n",
    "            #print(f'Training loss: {loss}')\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('/' * 20 + ' Validation ' + '/' * 20)\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            acc = []\n",
    "            for step, batch in enumerate(tqdm(valid_dl)):\n",
    "                input_p, target = batch\n",
    "                \n",
    "                output = net(input_p)\n",
    "\n",
    "                loss = loss_func(output, target).cpu()\n",
    "                val_loss.append(loss)\n",
    "\n",
    "                matching = [torch.argmax(i) == torch.argmax(j) for i, j in zip(output, target)]\n",
    "                accuracy = matching.count(True)/len(matching)\n",
    "                acc.append(accuracy)\n",
    "            wandb.log({'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'val_loss': np.mean(val_loss),\n",
    "                    'accuracy': np.mean(acc)})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7395/7395 [01:36<00:00, 76.44it/s]\n",
      "100%|██████████| 1849/1849 [00:23<00:00, 77.43it/s]\n"
     ]
    }
   ],
   "source": [
    "test_train_ds, test_valid_ds = generate_train_valid_set(test, env['aug'], env['val_pct'])\n",
    "test_train_dl, test_valid_dl = DataLoader(test_train_ds, batch_size=env['bs'], shuffle=True), DataLoader(test_valid_ds, batch_size=env['bs'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////// Epoch: 1 ////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:18<00:00, 100.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////// Validation ////////////////////\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/463 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_train_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_valid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(train_dl, valid_dl, env)\u001b[39m\n\u001b[32m     33\u001b[39m input_p, target = batch\n\u001b[32m     35\u001b[39m output = net(input_p)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m loss = \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m     38\u001b[39m val_loss.append(loss)\n\u001b[32m     40\u001b[39m matching = [torch.argmax(i) == torch.argmax(j) \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(output, target)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/SHREC/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/SHREC/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/SHREC/lib/python3.12/site-packages/torch/nn/modules/loss.py:1295\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/SHREC/lib/python3.12/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "train(test_train_dl, test_valid_dl, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SHREC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
